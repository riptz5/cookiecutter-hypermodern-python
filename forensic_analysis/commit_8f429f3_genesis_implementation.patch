commit 8f429f32f3e7f2bc5c3ee1652f580e535d6742c0
Author: riptz5 <leandroalvarez@gmail.com>
Date:   Fri Dec 26 22:55:23 2025 -0500

    feat(genesis): implement autopoietic multi-agent system
    
    GENESIS - Self-programming AI system for Google Cloud
    
    Core Modules:
    - genesis/core.py: GenesisCore orchestrator with PERCEIVEâ†’THINKâ†’ACTâ†’REMEMBERâ†’EVOLVE cycle
    - genesis/perceive.py: GCP environment scanning via plugin discovery
    - genesis/think.py: Gemini reasoning, ActionPlan generation, code generation
    - genesis/act.py: Action execution, agent/plugin generation
    - genesis/memory.py: Firestore persistence with local fallback
    - genesis/evolve.py: Self-improvement based on metrics analysis
    
    Cloud Infrastructure:
    - cloud/firestore.py: Async Firestore client
    - cloud/run.py: Cloud Run deployer
    - cloud/pubsub.py: Pub/Sub messaging
    
    Agent System:
    - agents/factory.py: AgentFactory for creating specialized agents
    - agents/bridge.py: ADK-LangGraph integration
    
    CLI Commands:
    - genesis cycle/run/status/evolve
    - ask, multi, discoify
    
    Tests & Examples:
    - Full test coverage for all new modules
    - Interactive genesis_example.py
    - Fixed gcp_discovery_example.py

diff --git a/{{cookiecutter.project_name}}/examples/gcp_discovery_example.py b/{{cookiecutter.project_name}}/examples/gcp_discovery_example.py
index ce21fc5..60146da 100644
--- a/{{cookiecutter.project_name}}/examples/gcp_discovery_example.py
+++ b/{{cookiecutter.project_name}}/examples/gcp_discovery_example.py
@@ -2,7 +2,7 @@
 """Example: Discover Google Cloud resources automatically.
 
 This script demonstrates automatic discovery of GCP resources using
-Application Default Credentials (ADC).
+Application Default Credentials (ADC) and the plugin-based discovery system.
 
 Setup:
 1. Install gcloud CLI: https://cloud.google.com/sdk/docs/install
@@ -14,8 +14,7 @@ No hardcoded credentials needed!
 """
 import asyncio
 import logging
-from {{cookiecutter.package_name}}.core.gcp_discovery import discover_gcp_resources
-
+from {{cookiecutter.package_name}}.core.gcp_discovery import discover_gcp_resources, GCPDiscovery
 
 # Configure logging
 logging.basicConfig(
@@ -24,10 +23,26 @@ logging.basicConfig(
 )
 
 
+def get_service_resources(resources, service_pattern: str):
+    """Helper to get resources for a service pattern.
+    
+    Args:
+        resources: GCPResources object
+        service_pattern: Pattern to match service name
+        
+    Returns:
+        Dict of resources or None
+    """
+    for service_name, data in resources.service_resources.items():
+        if service_pattern.lower() in service_name.lower():
+            return data
+    return None
+
+
 async def main():
     """Discover and display GCP resources."""
     print("\n" + "=" * 80)
-    print("GOOGLE CLOUD AUTOMATIC DISCOVERY")
+    print("GOOGLE CLOUD AUTOMATIC DISCOVERY (PLUGIN-BASED)")
     print("=" * 80)
     
     try:
@@ -39,46 +54,53 @@ async def main():
         print(f"ðŸ“ Region: {resources.project.region}")
         
         # Display enabled services
-        print(f"\nðŸ”§ Enabled Services ({len([s for s in resources.services if s.enabled])}):")
-        for service in sorted(resources.services, key=lambda s: s.display_name):
-            if service.enabled:
-                print(f"  {service}")
+        enabled_services = [s for s in resources.services if s.enabled]
+        print(f"\nðŸ”§ Enabled Services ({len(enabled_services)}):")
+        for service in sorted(enabled_services, key=lambda s: s.display_name)[:15]:
+            print(f"  {service}")
+        if len(enabled_services) > 15:
+            print(f"  ... and {len(enabled_services) - 15} more")
         
-        # Display secrets
-        if resources.secrets:
-            print(f"\nðŸ” Secrets ({len(resources.secrets)}):")
-            for secret in resources.secrets[:10]:  # Show first 10
-                print(f"  - {secret}")
-            if len(resources.secrets) > 10:
-                print(f"  ... and {len(resources.secrets) - 10} more")
+        # Display discovered resources dynamically
+        print(f"\nðŸ“‚ Discovered Resources ({len(resources.service_resources)} services):")
         
-        # Display storage buckets
-        if resources.storage_buckets:
-            print(f"\nðŸª£ Storage Buckets ({len(resources.storage_buckets)}):")
-            for bucket in resources.storage_buckets[:10]:
-                print(f"  - {bucket}")
-            if len(resources.storage_buckets) > 10:
-                print(f"  ... and {len(resources.storage_buckets) - 10} more")
-        
-        # Display Firestore collections
-        if resources.firestore_collections:
-            print(f"\nðŸ”¥ Firestore Collections ({len(resources.firestore_collections)}):")
-            for collection in resources.firestore_collections:
-                print(f"  - {collection}")
-        
-        # Display BigQuery datasets
-        if resources.bigquery_datasets:
-            print(f"\nðŸ“Š BigQuery Datasets ({len(resources.bigquery_datasets)}):")
-            for dataset in resources.bigquery_datasets:
-                print(f"  - {dataset}")
-        
-        # Display Vertex AI models
-        if resources.vertex_models:
-            print(f"\nðŸ¤– Vertex AI Models ({len(resources.vertex_models)}):")
-            for model in resources.vertex_models[:5]:
-                print(f"  - {model}")
-            if len(resources.vertex_models) > 5:
-                print(f"  ... and {len(resources.vertex_models) - 5} more")
+        for service_name, data in resources.service_resources.items():
+            # Extract clean service name
+            service_key = service_name.split('.')[-1].replace('.googleapis.com', '')
+            resource_type = data.get('type', 'resources')
+            count = data.get('count', 0)
+            
+            # Get emoji based on service
+            emoji_map = {
+                'secret': 'ðŸ”',
+                'storage': 'ðŸª£',
+                'firestore': 'ðŸ”¥',
+                'bigquery': 'ðŸ“Š',
+                'vertex': 'ðŸ¤–',
+                'compute': 'ðŸ–¥ï¸',
+                'run': 'ðŸƒ',
+                'pubsub': 'ðŸ“¨',
+                'spanner': 'ðŸ—„ï¸',
+            }
+            emoji = 'ðŸ“'
+            for pattern, e in emoji_map.items():
+                if pattern in service_key.lower():
+                    emoji = e
+                    break
+            
+            print(f"\n  {emoji} {service_key.title()} ({count} {resource_type}):")
+            
+            # Show resource items (limited)
+            items = data.get('resources', [])
+            for item in items[:5]:
+                if isinstance(item, dict):
+                    name = item.get('name', item.get('id', str(item)))
+                else:
+                    name = str(item)
+                print(f"     - {name}")
+            
+            if len(items) > 5:
+                print(f"     ... and {len(items) - 5} more")
         
         # Summary
         print("\n" + "=" * 80)
@@ -86,15 +108,38 @@ async def main():
         print("=" * 80)
         summary = resources.to_dict()
         for key, value in summary.items():
-            print(f"{key}: {value}")
+            if isinstance(value, list):
+                print(f"{key}: [{len(value)} items]")
+            else:
+                print(f"{key}: {value}")
         
         print("\nâœ… Discovery complete!")
         print("\nNext steps:")
         print("1. Use these resources in your agents")
-        print("2. Access secrets with: discovery.get_secret('secret-id')")
-        print("3. Connect to databases automatically")
+        print("2. Access resources via: discovery.get_service_resources('secretmanager')")
+        print("3. Add custom plugins for new services")
         print("4. Deploy to Cloud Run with zero config")
         
+        # Show usage example
+        print("\n" + "-" * 40)
+        print("Usage Example:")
+        print("-" * 40)
+        print("""
+>>> from {{cookiecutter.package_name}}.core.gcp_discovery import GCPDiscovery
+>>> 
+>>> discovery = GCPDiscovery()
+>>> resources = discovery.discover_all()
+>>> 
+>>> # Get specific service resources
+>>> secrets = discovery.get_service_resources('secretmanager')
+>>> if secrets:
+...     print(f"Found {secrets['count']} secrets")
+>>> 
+>>> # Iterate all services
+>>> for service, data in resources.service_resources.items():
+...     print(f"{service}: {data['count']} {data['type']}")
+""")
+        
     except Exception as e:
         print(f"\nâŒ Error: {e}")
         print("\nTroubleshooting:")
@@ -102,6 +147,9 @@ async def main():
         print("2. Run: gcloud config set project YOUR_PROJECT_ID")
         print("3. Enable required APIs in Cloud Console")
         print("4. Check IAM permissions")
+        
+        import traceback
+        traceback.print_exc()
 
 
 if __name__ == "__main__":
diff --git a/{{cookiecutter.project_name}}/examples/genesis_example.py b/{{cookiecutter.project_name}}/examples/genesis_example.py
new file mode 100644
index 0000000..42c9a0e
--- /dev/null
+++ b/{{cookiecutter.project_name}}/examples/genesis_example.py
@@ -0,0 +1,243 @@
+{%- if cookiecutter.use_google_adk == 'y' and cookiecutter.use_google_cloud == 'y' %}
+"""Example: Run GENESIS autopoietic system.
+
+This script demonstrates the GENESIS autopoietic system that:
+1. PERCEIVES its GCP environment automatically
+2. THINKS about what actions to take using Gemini
+3. ACTS by generating code and executing actions
+4. REMEMBERS its state in Firestore
+5. EVOLVES by improving its own code
+
+Setup:
+1. Set GOOGLE_API_KEY environment variable
+2. Optionally set GOOGLE_CLOUD_PROJECT
+3. Authenticate: gcloud auth application-default login
+4. Run this script
+
+GENESIS will autonomously discover your GCP resources and
+propose actions to improve itself.
+"""
+import asyncio
+import logging
+import os
+import sys
+
+# Configure logging
+logging.basicConfig(
+    level=logging.INFO,
+    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
+    datefmt='%Y-%m-%d %H:%M:%S',
+)
+logger = logging.getLogger("genesis_example")
+
+
+async def run_single_cycle():
+    """Run a single GENESIS cycle."""
+    from {{cookiecutter.package_name}}.genesis import GenesisCore
+    
+    print("\n" + "=" * 80)
+    print("GENESIS - SINGLE CYCLE EXECUTION")
+    print("=" * 80)
+    
+    # Initialize GENESIS
+    logger.info("Initializing GENESIS Core...")
+    genesis = GenesisCore(
+        evolution_threshold=10,  # Evolve every 10 cycles
+        auto_evolve=True,
+    )
+    
+    # Run a cycle
+    logger.info("Running cycle...")
+    result = await genesis.run_cycle()
+    
+    # Display results
+    print("\n" + "-" * 40)
+    print("CYCLE RESULT")
+    print("-" * 40)
+    print(f"  Cycle ID: {result.cycle_id}")
+    print(f"  Success: {result.success}")
+    print(f"  Duration: {result.duration_ms:.2f}ms")
+    print(f"  Context Hash: {result.context_hash}")
+    print(f"  Actions: {len(result.actions_taken)}")
+    print(f"  Evolved: {result.evolved}")
+    
+    if result.actions_taken:
+        print("\n  Actions taken:")
+        for action in result.actions_taken:
+            print(f"    - {action}")
+    
+    if result.errors:
+        print(f"\n  Errors ({len(result.errors)}):")
+        for error in result.errors[:5]:
+            print(f"    - {error}")
+    
+    if result.plan_summary:
+        print(f"\n  Plan summary: {result.plan_summary[:200]}...")
+    
+    # Show system status
+    status = genesis.get_status()
+    print("\n" + "-" * 40)
+    print("SYSTEM STATUS")
+    print("-" * 40)
+    for key, value in status.items():
+        print(f"  {key}: {value}")
+    
+    return result
+
+
+async def run_with_task(task: str):
+    """Run GENESIS with a specific task."""
+    from {{cookiecutter.package_name}}.genesis import GenesisCore
+    
+    print("\n" + "=" * 80)
+    print(f"GENESIS - TASK: {task[:50]}...")
+    print("=" * 80)
+    
+    genesis = GenesisCore()
+    result = await genesis.run_cycle(task=task)
+    
+    print(f"\nResult: {'SUCCESS' if result.success else 'FAILED'}")
+    print(f"Actions: {result.actions_taken}")
+    
+    return result
+
+
+async def run_continuous(cycles: int = 3, interval: int = 30):
+    """Run GENESIS continuously for N cycles."""
+    from {{cookiecutter.package_name}}.genesis import GenesisCore
+    
+    print("\n" + "=" * 80)
+    print(f"GENESIS - CONTINUOUS MODE ({cycles} cycles)")
+    print("=" * 80)
+    
+    genesis = GenesisCore(evolution_threshold=2)  # Lower for demo
+    
+    try:
+        await genesis.run_continuous(
+            interval_seconds=interval,
+            max_cycles=cycles,
+        )
+    except KeyboardInterrupt:
+        print("\nStopped by user.")
+    
+    status = genesis.get_status()
+    print(f"\nCompleted {status['cycles_completed']} cycles")
+    
+    return status
+
+
+async def demo_modules():
+    """Demonstrate individual GENESIS modules."""
+    print("\n" + "=" * 80)
+    print("GENESIS MODULE DEMO")
+    print("=" * 80)
+    
+    # 1. Perceive Module
+    print("\n1. PERCEIVE MODULE")
+    print("-" * 40)
+    
+    from {{cookiecutter.package_name}}.genesis.perceive import PerceiveModule
+    
+    perceive = PerceiveModule()
+    context = await perceive.scan()
+    
+    print(f"  Project: {context.project_id}")
+    print(f"  Region: {context.region}")
+    print(f"  Services: {len(context.services)}")
+    print(f"  Resources: {len(context.resources)}")
+    print(f"  Changes: {len(context.changes)}")
+    
+    # 2. Think Module (requires GOOGLE_API_KEY)
+    print("\n2. THINK MODULE")
+    print("-" * 40)
+    
+    if os.getenv("GOOGLE_API_KEY"):
+        from {{cookiecutter.package_name}}.genesis.think import ThinkModule
+        
+        think = ThinkModule()
+        
+        # Test JSON extraction
+        test_json = '{"reasoning": "test", "actions": []}'
+        extracted = think._extract_json(test_json)
+        print(f"  JSON extraction: {'OK' if extracted else 'FAILED'}")
+        
+        # Test syntax validation
+        try:
+            think._validate_syntax("def test(): pass")
+            print("  Syntax validation: OK")
+        except SyntaxError:
+            print("  Syntax validation: FAILED")
+    else:
+        print("  Skipped (GOOGLE_API_KEY not set)")
+    
+    # 3. Memory Module
+    print("\n3. MEMORY MODULE")
+    print("-" * 40)
+    
+    from {{cookiecutter.package_name}}.genesis.memory import MemoryModule
+    
+    memory = MemoryModule()
+    memory._use_local = True  # Force local for demo
+    
+    state = await memory.get_state()
+    print(f"  Total cycles: {state.total_cycles}")
+    print(f"  Success rate: {state.success_rate:.1%}")
+    print(f"  Using local cache: {memory._use_local}")
+    
+    print("\nAll modules functional!")
+
+
+async def main():
+    """Main entry point."""
+    print("\n" + "=" * 80)
+    print("GENESIS AUTOPOIETIC SYSTEM - EXAMPLE")
+    print("=" * 80)
+    
+    # Check prerequisites
+    api_key = os.getenv("GOOGLE_API_KEY")
+    project = os.getenv("GOOGLE_CLOUD_PROJECT")
+    
+    print("\nPrerequisites:")
+    print(f"  GOOGLE_API_KEY: {'SET' if api_key else 'NOT SET'}")
+    print(f"  GOOGLE_CLOUD_PROJECT: {project or 'NOT SET'}")
+    
+    if not api_key:
+        print("\nâš ï¸  GOOGLE_API_KEY not set!")
+        print("Set it with: export GOOGLE_API_KEY=your_key")
+        print("\nRunning in demo mode (modules only)...\n")
+        await demo_modules()
+        return
+    
+    # Menu
+    print("\nOptions:")
+    print("  1. Run single cycle")
+    print("  2. Run with specific task")
+    print("  3. Run continuous (3 cycles)")
+    print("  4. Demo individual modules")
+    print("  5. Exit")
+    
+    choice = input("\nSelect option (1-5): ").strip()
+    
+    if choice == "1":
+        await run_single_cycle()
+    elif choice == "2":
+        task = input("Enter task: ").strip()
+        if task:
+            await run_with_task(task)
+    elif choice == "3":
+        await run_continuous(cycles=3, interval=10)
+    elif choice == "4":
+        await demo_modules()
+    elif choice == "5":
+        print("Goodbye!")
+        return
+    else:
+        print("Invalid option. Running single cycle...")
+        await run_single_cycle()
+    
+    print("\nâœ… Example complete!")
+
+
+if __name__ == "__main__":
+    asyncio.run(main())
+{%- endif %}
diff --git a/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/__main__.py b/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/__main__.py
index ac354b2..8d6ed78 100644
--- a/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/__main__.py
+++ b/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/__main__.py
@@ -1,11 +1,342 @@
 """Command-line interface."""
+import asyncio
+import logging
+import sys
+
 import click
 
 
-@click.command()
+def setup_logging(verbose: bool = False) -> None:
+    """Configure logging based on verbosity."""
+    level = logging.DEBUG if verbose else logging.INFO
+    logging.basicConfig(
+        level=level,
+        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
+        datefmt="%Y-%m-%d %H:%M:%S",
+    )
+
+
+@click.group()
 @click.version_option()
-def main() -> None:
-    """{{cookiecutter.friendly_name}}."""
+@click.option("-v", "--verbose", is_flag=True, help="Enable verbose logging")
+@click.pass_context
+def main(ctx: click.Context, verbose: bool) -> None:
+    """{{cookiecutter.friendly_name}} - AI-powered multi-agent system."""
+    ctx.ensure_object(dict)
+    ctx.obj["verbose"] = verbose
+    setup_logging(verbose)
+
+
+@main.command()
+@click.pass_context
+def info(ctx: click.Context) -> None:
+    """Show system information."""
+    click.echo("{{cookiecutter.friendly_name}} v{{cookiecutter.version}}")
+    click.echo("=" * 40)
+    
+    # Check available features
+    features = []
+    {%- if cookiecutter.use_google_adk == 'y' %}
+    features.append("Google ADK")
+    {%- endif %}
+    {%- if cookiecutter.use_langgraph == 'y' %}
+    features.append("LangGraph")
+    {%- endif %}
+    {%- if cookiecutter.use_google_cloud == 'y' %}
+    features.append("Google Cloud")
+    {%- endif %}
+    
+    if features:
+        click.echo(f"Features: {', '.join(features)}")
+    else:
+        click.echo("Features: Base configuration")
+
+
+{%- if cookiecutter.use_google_adk == 'y' %}
+
+
+@main.command()
+@click.argument("prompt")
+@click.option("--model", default="gemini-2.0-flash-exp", help="Model to use")
+@click.pass_context
+def ask(ctx: click.Context, prompt: str, model: str) -> None:
+    """Ask a question using the ADK agent."""
+    async def _ask():
+        from .agents.adk import GoogleADKAgent, ADKConfig
+        
+        config = ADKConfig(model=model)
+        agent = GoogleADKAgent(config)
+        
+        click.echo(f"Asking: {prompt[:50]}...")
+        response = await agent.run(prompt)
+        click.echo("\nResponse:")
+        click.echo("-" * 40)
+        click.echo(response)
+    
+    asyncio.run(_ask())
+
+
+@main.command()
+@click.argument("task")
+@click.option("--workers", "-w", multiple=True, help="Worker types to use")
+@click.pass_context
+def multi(ctx: click.Context, task: str, workers: tuple) -> None:
+    """Run a task using multiple agents in parallel."""
+    async def _multi():
+        from .agents.factory import AgentFactory, AgentType
+        
+        factory = AgentFactory()
+        
+        # Default workers if none specified
+        if not workers:
+            worker_types = [AgentType.RESEARCH, AgentType.ANALYSIS]
+        else:
+            worker_types = [AgentType(w) for w in workers]
+        
+        click.echo(f"Running task with {len(worker_types)} workers...")
+        
+        # Create workers
+        agents = {}
+        for wt in worker_types:
+            agents[wt.value] = factory.create(f"worker-{wt.value}", wt)
+        
+        # Run in parallel
+        tasks = {
+            name: asyncio.create_task(agent.run(task))
+            for name, agent in agents.items()
+        }
+        
+        results = {}
+        for name, t in tasks.items():
+            try:
+                results[name] = await t
+                click.echo(f"\n[{name.upper()}]")
+                click.echo("-" * 40)
+                click.echo(results[name][:500] + "..." if len(results[name]) > 500 else results[name])
+            except Exception as e:
+                click.echo(f"\n[{name.upper()}] Error: {e}")
+        
+        click.echo(f"\nCompleted {len(results)} worker responses.")
+    
+    asyncio.run(_multi())
+{%- endif %}
+
+
+{%- if cookiecutter.use_google_adk == 'y' and cookiecutter.use_google_cloud == 'y' %}
+
+
+@main.group()
+@click.pass_context
+def genesis(ctx: click.Context) -> None:
+    """GENESIS autopoietic system commands."""
+    pass
+
+
+@genesis.command()
+@click.option("--task", "-t", default=None, help="Specific task to execute")
+@click.pass_context
+def cycle(ctx: click.Context, task: str) -> None:
+    """Run a single GENESIS cycle."""
+    async def _cycle():
+        from .genesis import GenesisCore
+        
+        click.echo("Initializing GENESIS Core...")
+        core = GenesisCore()
+        
+        click.echo("Running cycle...")
+        result = await core.run_cycle(task=task)
+        
+        click.echo("\nCycle Result:")
+        click.echo("-" * 40)
+        click.echo(f"Cycle ID: {result.cycle_id}")
+        click.echo(f"Success: {result.success}")
+        click.echo(f"Actions: {len(result.actions_taken)}")
+        click.echo(f"Duration: {result.duration_ms:.2f}ms")
+        click.echo(f"Evolved: {result.evolved}")
+        
+        if result.errors:
+            click.echo(f"\nErrors ({len(result.errors)}):")
+            for err in result.errors[:5]:
+                click.echo(f"  - {err}")
+        
+        if result.actions_taken:
+            click.echo(f"\nActions taken:")
+            for action in result.actions_taken[:10]:
+                click.echo(f"  - {action}")
+    
+    asyncio.run(_cycle())
+
+
+@genesis.command()
+@click.option("--interval", "-i", default=60, help="Seconds between cycles")
+@click.option("--max-cycles", "-m", default=None, type=int, help="Max cycles to run")
+@click.pass_context
+def run(ctx: click.Context, interval: int, max_cycles: int) -> None:
+    """Run GENESIS continuously."""
+    async def _run():
+        from .genesis import GenesisCore
+        
+        click.echo("Starting GENESIS in continuous mode...")
+        click.echo(f"Interval: {interval}s, Max cycles: {max_cycles or 'unlimited'}")
+        click.echo("Press Ctrl+C to stop.\n")
+        
+        core = GenesisCore()
+        
+        try:
+            await core.run_continuous(
+                interval_seconds=interval,
+                max_cycles=max_cycles,
+            )
+        except KeyboardInterrupt:
+            click.echo("\nStopping GENESIS...")
+        
+        status = core.get_status()
+        click.echo(f"\nFinal status:")
+        click.echo(f"  Cycles completed: {status['cycles_completed']}")
+        click.echo(f"  Uptime: {status['uptime_seconds']:.0f}s")
+    
+    asyncio.run(_run())
+
+
+@genesis.command()
+@click.pass_context
+def status(ctx: click.Context) -> None:
+    """Show GENESIS system status."""
+    async def _status():
+        from .genesis import GenesisCore, MemoryModule
+        
+        click.echo("GENESIS System Status")
+        click.echo("=" * 40)
+        
+        # Try to get memory state
+        memory = MemoryModule()
+        try:
+            state = await memory.get_state()
+            click.echo(f"Total cycles: {state.total_cycles}")
+            click.echo(f"Success rate: {state.success_rate:.1%}")
+            click.echo(f"Agents generated: {state.agents_generated}")
+            click.echo(f"Plugins generated: {state.plugins_generated}")
+            
+            if state.errors_recent:
+                click.echo(f"\nRecent errors ({len(state.errors_recent)}):")
+                for err in state.errors_recent[:3]:
+                    click.echo(f"  - {err[:60]}...")
+        except Exception as e:
+            click.echo(f"Could not retrieve memory state: {e}")
+            click.echo("(Memory may not be initialized yet)")
+    
+    asyncio.run(_status())
+
+
+@genesis.command()
+@click.pass_context
+def evolve(ctx: click.Context) -> None:
+    """Force an evolution cycle."""
+    async def _evolve():
+        from .genesis import GenesisCore
+        
+        click.echo("Forcing GENESIS evolution...")
+        core = GenesisCore()
+        
+        success = await core.force_evolve()
+        
+        if success:
+            click.echo("Evolution completed successfully!")
+        else:
+            click.echo("Evolution failed. Check logs for details.")
+    
+    asyncio.run(_evolve())
+{%- endif %}
+
+
+{%- if cookiecutter.use_google_cloud == 'y' %}
+
+
+@main.command()
+@click.pass_context
+def discover(ctx: click.Context) -> None:
+    """Discover GCP resources in the project."""
+    from .core.gcp_discovery import GCPDiscovery
+    
+    click.echo("Discovering GCP resources...")
+    discovery = GCPDiscovery()
+    
+    # Discover project
+    project = discovery.discover_project()
+    click.echo(f"\nProject: {project.project_id}")
+    click.echo(f"Region: {project.region}")
+    
+    # Discover services
+    services = discovery.discover_enabled_services()
+    enabled = [s for s in services if s.enabled]
+    click.echo(f"\nEnabled services: {len(enabled)}")
+    for svc in enabled[:10]:
+        click.echo(f"  - {svc.name}")
+    if len(enabled) > 10:
+        click.echo(f"  ... and {len(enabled) - 10} more")
+    
+    # Discover resources
+    resources = discovery.discover_all_service_resources()
+    click.echo(f"\nResources by service:")
+    for service, data in resources.items():
+        count = data.get("count", 0)
+        if count > 0:
+            click.echo(f"  - {service}: {count}")
+
+
+@main.command()
+@click.pass_context
+def verify(ctx: click.Context) -> None:
+    """Verify the production setup."""
+    import os
+    
+    click.echo("Verifying production setup...")
+    click.echo("=" * 40)
+    
+    checks = []
+    
+    # Check API key
+    api_key = os.getenv("GOOGLE_API_KEY")
+    checks.append(("GOOGLE_API_KEY", "set" if api_key else "missing"))
+    
+    # Check project
+    project = os.getenv("GOOGLE_CLOUD_PROJECT")
+    checks.append(("GOOGLE_CLOUD_PROJECT", project or "not set"))
+    
+    # Check imports
+    try:
+        from google import genai
+        checks.append(("google-genai", "installed"))
+    except ImportError:
+        checks.append(("google-genai", "not installed"))
+    
+    try:
+        from google.cloud import firestore
+        checks.append(("google-cloud-firestore", "installed"))
+    except ImportError:
+        checks.append(("google-cloud-firestore", "not installed"))
+    
+    try:
+        from langgraph.graph import StateGraph
+        checks.append(("langgraph", "installed"))
+    except ImportError:
+        checks.append(("langgraph", "not installed"))
+    
+    # Print results
+    all_ok = True
+    for name, status in checks:
+        icon = "âœ“" if status not in ["missing", "not installed", "not set"] else "âœ—"
+        if icon == "âœ—":
+            all_ok = False
+        click.echo(f"  {icon} {name}: {status}")
+    
+    click.echo()
+    if all_ok:
+        click.echo("All checks passed! System ready for production.")
+    else:
+        click.echo("Some checks failed. Review configuration.")
+        sys.exit(1)
+{%- endif %}
 
 
 if __name__ == "__main__":
diff --git a/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/agents/bridge.py b/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/agents/bridge.py
index 05102a9..92b6443 100644
--- a/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/agents/bridge.py
+++ b/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/agents/bridge.py
@@ -1 +1,346 @@
-"""A2A/MCP bridge for agent communication."""
+{%- if cookiecutter.use_google_adk == 'y' and cookiecutter.use_langgraph == 'y' %}
+"""ADK-LangGraph Bridge - Conecta agentes ADK con LangGraph.
+
+Este modulo proporciona utilidades para integrar agentes
+GoogleADK como nodos en grafos de LangGraph, permitiendo
+orquestacion sofisticada de multi-agentes.
+"""
+import asyncio
+import logging
+import os
+from typing import Callable, Dict, Any, List, Optional
+from dataclasses import dataclass, field
+
+logger = logging.getLogger(__name__)
+
+
+@dataclass
+class BridgeConfig:
+    """Configuracion del bridge.
+    
+    Attributes:
+        api_key: API key de Google
+        default_model: Modelo por defecto
+        max_parallel: Max agentes en paralelo
+        timeout_seconds: Timeout por agente
+    """
+    api_key: Optional[str] = None
+    default_model: str = "gemini-2.0-flash-exp"
+    max_parallel: int = 5
+    timeout_seconds: float = 120.0
+    
+    def __post_init__(self):
+        if self.api_key is None:
+            self.api_key = os.getenv("GOOGLE_API_KEY")
+
+
+class ADKLangGraphBridge:
+    """Bridge entre Google ADK y LangGraph.
+    
+    Permite usar agentes ADK como nodos en grafos LangGraph,
+    facilitando la construccion de sistemas multi-agente
+    con orquestacion sofisticada.
+    
+    Example:
+        >>> bridge = ADKLangGraphBridge()
+        >>> 
+        >>> # Crear nodo desde agente ADK
+        >>> agent = GoogleADKAgent(config)
+        >>> node_fn = bridge.wrap_adk_agent(agent)
+        >>> 
+        >>> # Usar en grafo LangGraph
+        >>> graph = StateGraph(AgentState)
+        >>> graph.add_node("researcher", node_fn)
+    """
+    
+    def __init__(self, config: Optional[BridgeConfig] = None):
+        """Inicializa el bridge.
+        
+        Args:
+            config: Configuracion del bridge
+        """
+        self.config = config or BridgeConfig()
+        self._wrapped_agents: Dict[str, Callable] = {}
+        logger.info("ADKLangGraphBridge initialized")
+    
+    def wrap_adk_agent(
+        self,
+        agent,
+        state_key: str = "messages",
+        output_key: str = "agent_response",
+    ) -> Callable:
+        """Envuelve un agente ADK como nodo LangGraph.
+        
+        Args:
+            agent: Instancia de GoogleADKAgent
+            state_key: Key en el state para obtener el prompt
+            output_key: Key donde guardar la respuesta
+            
+        Returns:
+            Funcion async compatible con LangGraph
+        """
+        async def node_fn(state: Dict[str, Any]) -> Dict[str, Any]:
+            """Nodo LangGraph que ejecuta el agente ADK."""
+            # Extraer prompt del state
+            prompt = self._extract_prompt(state, state_key)
+            
+            if not prompt:
+                logger.warning("No prompt found in state")
+                return {output_key: "No prompt provided", "error": True}
+            
+            try:
+                # Ejecutar agente con timeout
+                response = await asyncio.wait_for(
+                    agent.run(prompt),
+                    timeout=self.config.timeout_seconds,
+                )
+                
+                return {
+                    output_key: response,
+                    "agent_name": getattr(agent, "name", "unknown"),
+                    "error": False,
+                }
+                
+            except asyncio.TimeoutError:
+                logger.error(f"Agent timeout after {self.config.timeout_seconds}s")
+                return {output_key: "Agent timeout", "error": True}
+            except Exception as e:
+                logger.error(f"Agent error: {e}")
+                return {output_key: str(e), "error": True}
+        
+        return node_fn
+    
+    def wrap_multiple(
+        self,
+        agents: Dict[str, Any],
+        state_key: str = "task",
+    ) -> Dict[str, Callable]:
+        """Envuelve multiples agentes como nodos.
+        
+        Args:
+            agents: Dict de nombre -> agente
+            state_key: Key del state para el prompt
+            
+        Returns:
+            Dict de nombre -> node function
+        """
+        nodes = {}
+        for name, agent in agents.items():
+            nodes[name] = self.wrap_adk_agent(
+                agent,
+                state_key=state_key,
+                output_key=f"{name}_response",
+            )
+            self._wrapped_agents[name] = nodes[name]
+        
+        logger.info(f"Wrapped {len(nodes)} agents as LangGraph nodes")
+        return nodes
+    
+    def create_parallel_node(
+        self,
+        agents: Dict[str, Any],
+        state_key: str = "task",
+    ) -> Callable:
+        """Crea un nodo que ejecuta multiples agentes en paralelo.
+        
+        Args:
+            agents: Dict de nombre -> agente
+            state_key: Key del state para el prompt
+            
+        Returns:
+            Funcion async que ejecuta todos en paralelo
+        """
+        async def parallel_node(state: Dict[str, Any]) -> Dict[str, Any]:
+            """Ejecuta todos los agentes en paralelo."""
+            prompt = self._extract_prompt(state, state_key)
+            
+            if not prompt:
+                return {"results": {}, "error": True}
+            
+            # Crear tareas para todos los agentes
+            tasks = {}
+            for name, agent in agents.items():
+                tasks[name] = asyncio.create_task(
+                    self._run_with_timeout(agent, prompt)
+                )
+            
+            # Esperar todos
+            results = {}
+            for name, task in tasks.items():
+                try:
+                    results[name] = await task
+                except Exception as e:
+                    results[name] = f"Error: {e}"
+            
+            return {
+                "results": results,
+                "agents_run": list(results.keys()),
+                "error": False,
+            }
+        
+        return parallel_node
+    
+    def create_sequential_node(
+        self,
+        agents: List[Any],
+        state_key: str = "task",
+    ) -> Callable:
+        """Crea un nodo que ejecuta agentes secuencialmente.
+        
+        Cada agente recibe el output del anterior.
+        
+        Args:
+            agents: Lista ordenada de agentes
+            state_key: Key del state para el prompt inicial
+            
+        Returns:
+            Funcion async que ejecuta en secuencia
+        """
+        async def sequential_node(state: Dict[str, Any]) -> Dict[str, Any]:
+            """Ejecuta agentes en secuencia."""
+            current_input = self._extract_prompt(state, state_key)
+            results = []
+            
+            for i, agent in enumerate(agents):
+                try:
+                    response = await self._run_with_timeout(agent, current_input)
+                    results.append(response)
+                    # El output se convierte en input del siguiente
+                    current_input = response
+                except Exception as e:
+                    results.append(f"Error at step {i}: {e}")
+                    break
+            
+            return {
+                "final_output": results[-1] if results else "",
+                "intermediate_results": results,
+                "steps_completed": len(results),
+            }
+        
+        return sequential_node
+    
+    def create_router_node(
+        self,
+        agents: Dict[str, Any],
+        router_agent: Any,
+    ) -> Callable:
+        """Crea un nodo que routea tareas a agentes especificos.
+        
+        Args:
+            agents: Dict de nombre -> agente
+            router_agent: Agente que decide a quien routear
+            
+        Returns:
+            Funcion async que routea y ejecuta
+        """
+        async def router_node(state: Dict[str, Any]) -> Dict[str, Any]:
+            """Routea tarea al agente apropiado."""
+            task = self._extract_prompt(state, "task")
+            
+            # Pedir al router que decida
+            routing_prompt = f"""Analiza la siguiente tarea y decide que agente debe manejarla.
+
+Agentes disponibles: {list(agents.keys())}
+
+Tarea: {task}
+
+Responde SOLO con el nombre del agente (una palabra).
+"""
+            
+            try:
+                router_response = await self._run_with_timeout(router_agent, routing_prompt)
+                selected_agent = router_response.strip().lower()
+                
+                # Buscar agente
+                agent = None
+                for name, a in agents.items():
+                    if name.lower() in selected_agent or selected_agent in name.lower():
+                        agent = a
+                        selected_agent = name
+                        break
+                
+                if agent is None:
+                    # Default al primero
+                    selected_agent = list(agents.keys())[0]
+                    agent = agents[selected_agent]
+                
+                # Ejecutar agente seleccionado
+                result = await self._run_with_timeout(agent, task)
+                
+                return {
+                    "routed_to": selected_agent,
+                    "agent_response": result,
+                    "routing_decision": router_response,
+                }
+                
+            except Exception as e:
+                return {
+                    "error": True,
+                    "message": str(e),
+                }
+        
+        return router_node
+    
+    async def _run_with_timeout(self, agent, prompt: str) -> str:
+        """Ejecuta agente con timeout.
+        
+        Args:
+            agent: Agente a ejecutar
+            prompt: Prompt para el agente
+            
+        Returns:
+            Respuesta del agente
+        """
+        return await asyncio.wait_for(
+            agent.run(prompt),
+            timeout=self.config.timeout_seconds,
+        )
+    
+    def _extract_prompt(self, state: Dict[str, Any], key: str) -> str:
+        """Extrae prompt del state.
+        
+        Args:
+            state: State de LangGraph
+            key: Key a buscar
+            
+        Returns:
+            Prompt como string
+        """
+        value = state.get(key)
+        
+        if isinstance(value, str):
+            return value
+        
+        if isinstance(value, list):
+            # Asumir lista de mensajes, tomar el ultimo
+            if value:
+                last = value[-1]
+                if isinstance(last, dict):
+                    return last.get("content", str(last))
+                return str(last)
+            return ""
+        
+        if isinstance(value, dict):
+            return value.get("content", str(value))
+        
+        return str(value) if value else ""
+
+
+def create_bridge(api_key: Optional[str] = None) -> ADKLangGraphBridge:
+    """Crea un bridge con configuracion por defecto.
+    
+    Args:
+        api_key: API key de Google (opcional)
+        
+    Returns:
+        ADKLangGraphBridge configurado
+    """
+    config = BridgeConfig(api_key=api_key)
+    return ADKLangGraphBridge(config)
+{%- else %}
+"""A2A/MCP bridge for agent communication.
+
+Este modulo requiere use_google_adk='y' y use_langgraph='y'
+para funcionalidad completa.
+"""
+{%- endif %}
diff --git a/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/agents/factory.py b/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/agents/factory.py
new file mode 100644
index 0000000..d2f63a6
--- /dev/null
+++ b/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/agents/factory.py
@@ -0,0 +1,335 @@
+{%- if cookiecutter.use_google_adk == 'y' %}
+"""Agent Factory - Auto-genera agentes para servicios GCP.
+
+Este modulo proporciona una fabrica para crear agentes
+especializados automaticamente basandose en el servicio
+GCP objetivo.
+"""
+import logging
+from typing import Dict, Any, Optional, Type, List
+from dataclasses import dataclass, field
+from enum import Enum
+
+logger = logging.getLogger(__name__)
+
+
+class AgentType(Enum):
+    """Tipos de agentes disponibles."""
+    RESEARCH = "research"
+    ANALYSIS = "analysis"
+    WRITER = "writer"
+    CODE = "code"
+    DATA = "data"
+    INFRASTRUCTURE = "infrastructure"
+    MONITORING = "monitoring"
+
+
+@dataclass
+class AgentSpec:
+    """Especificacion de un agente.
+    
+    Attributes:
+        name: Nombre unico del agente
+        agent_type: Tipo de agente
+        target_service: Servicio GCP objetivo
+        capabilities: Lista de capacidades
+        system_prompt: Prompt de sistema personalizado
+        model: Modelo a usar
+        temperature: Temperatura del modelo
+    """
+    name: str
+    agent_type: AgentType
+    target_service: str = ""
+    capabilities: List[str] = field(default_factory=list)
+    system_prompt: Optional[str] = None
+    model: str = "gemini-2.0-flash-exp"
+    temperature: float = 0.7
+
+
+# Templates de system prompts por tipo
+AGENT_PROMPTS = {
+    AgentType.RESEARCH: """Eres un agente de investigacion especializado.
+Tu objetivo es buscar, analizar y sintetizar informacion sobre {target_service}.
+Proporciona respuestas bien estructuradas y citando fuentes cuando sea posible.
+Enfocate en hechos verificables y datos actuales.""",
+
+    AgentType.ANALYSIS: """Eres un agente de analisis especializado.
+Tu objetivo es analizar datos y patrones relacionados con {target_service}.
+Proporciona analisis profundos con conclusiones accionables.
+Usa metricas cuantitativas cuando sea posible.""",
+
+    AgentType.WRITER: """Eres un agente escritor especializado.
+Tu objetivo es generar contenido de alta calidad sobre {target_service}.
+Escribe de forma clara, concisa y profesional.
+Adapta el tono al contexto solicitado.""",
+
+    AgentType.CODE: """Eres un agente de desarrollo de codigo especializado.
+Tu objetivo es generar, revisar y mejorar codigo para {target_service}.
+Sigue las mejores practicas de Python y PEP8.
+Incluye type hints, docstrings y manejo de errores robusto.""",
+
+    AgentType.DATA: """Eres un agente de datos especializado.
+Tu objetivo es manejar operaciones de datos en {target_service}.
+Optimiza consultas, transforma datos y asegura calidad.
+Prioriza eficiencia y escalabilidad.""",
+
+    AgentType.INFRASTRUCTURE: """Eres un agente de infraestructura especializado.
+Tu objetivo es gestionar recursos de {target_service}.
+Aplica IaC best practices y optimiza costos.
+Prioriza seguridad y disponibilidad.""",
+
+    AgentType.MONITORING: """Eres un agente de monitoreo especializado.
+Tu objetivo es observar y reportar sobre {target_service}.
+Detecta anomalias, genera alertas y propone remediaciones.
+Mantien dashboards y metricas actualizados.""",
+}
+
+
+# Mapeo de servicios GCP a tipos de agentes recomendados
+SERVICE_AGENT_MAPPING = {
+    "bigquery": [AgentType.DATA, AgentType.ANALYSIS],
+    "storage": [AgentType.DATA, AgentType.INFRASTRUCTURE],
+    "compute": [AgentType.INFRASTRUCTURE, AgentType.MONITORING],
+    "run": [AgentType.INFRASTRUCTURE, AgentType.CODE],
+    "functions": [AgentType.CODE, AgentType.INFRASTRUCTURE],
+    "pubsub": [AgentType.DATA, AgentType.INFRASTRUCTURE],
+    "firestore": [AgentType.DATA, AgentType.CODE],
+    "spanner": [AgentType.DATA, AgentType.ANALYSIS],
+    "vertexai": [AgentType.RESEARCH, AgentType.ANALYSIS],
+    "monitoring": [AgentType.MONITORING, AgentType.ANALYSIS],
+    "logging": [AgentType.MONITORING, AgentType.ANALYSIS],
+}
+
+
+class AgentFactory:
+    """Fabrica de agentes especializados.
+    
+    Crea agentes automaticamente basandose en el servicio
+    objetivo y el tipo de tarea requerida.
+    
+    Example:
+        >>> factory = AgentFactory()
+        >>> agent = factory.create("bigquery-analyst", AgentType.DATA, "bigquery")
+        >>> result = await agent.run("Analiza las tablas disponibles")
+    """
+    
+    def __init__(self, api_key: Optional[str] = None):
+        """Inicializa la fabrica.
+        
+        Args:
+            api_key: API key de Google (usa GOOGLE_API_KEY si es None)
+        """
+        import os
+        self._api_key = api_key or os.getenv("GOOGLE_API_KEY")
+        self._agents: Dict[str, Any] = {}
+        self._specs: Dict[str, AgentSpec] = {}
+        logger.info("AgentFactory initialized")
+    
+    def create(
+        self,
+        name: str,
+        agent_type: AgentType,
+        target_service: str = "",
+        capabilities: Optional[List[str]] = None,
+        custom_prompt: Optional[str] = None,
+        **kwargs,
+    ) -> Any:
+        """Crea un nuevo agente.
+        
+        Args:
+            name: Nombre unico del agente
+            agent_type: Tipo de agente
+            target_service: Servicio GCP objetivo
+            capabilities: Lista de capacidades
+            custom_prompt: Prompt personalizado (override default)
+            **kwargs: Args adicionales para el agente
+            
+        Returns:
+            Instancia del agente creado
+        """
+        if name in self._agents:
+            logger.debug(f"Returning cached agent: {name}")
+            return self._agents[name]
+        
+        # Construir system prompt
+        if custom_prompt:
+            system_prompt = custom_prompt
+        else:
+            template = AGENT_PROMPTS.get(agent_type, AGENT_PROMPTS[AgentType.RESEARCH])
+            system_prompt = template.format(target_service=target_service or "GCP")
+        
+        # Crear spec
+        spec = AgentSpec(
+            name=name,
+            agent_type=agent_type,
+            target_service=target_service,
+            capabilities=capabilities or [],
+            system_prompt=system_prompt,
+            model=kwargs.get("model", "gemini-2.0-flash-exp"),
+            temperature=kwargs.get("temperature", 0.7),
+        )
+        
+        # Crear agente
+        try:
+            from .adk import GoogleADKAgent, ADKConfig
+            
+            config = ADKConfig(
+                model=spec.model,
+                temperature=spec.temperature,
+                system_instruction=spec.system_prompt,
+                api_key=self._api_key,
+            )
+            
+            agent = GoogleADKAgent(config)
+            agent.name = name
+            agent.spec = spec
+            
+            # Cachear
+            self._agents[name] = agent
+            self._specs[name] = spec
+            
+            logger.info(f"Created agent: {name} ({agent_type.value})")
+            return agent
+            
+        except ImportError as e:
+            logger.error(f"Failed to create agent: {e}")
+            raise
+    
+    def create_for_service(
+        self,
+        service: str,
+        agent_type: Optional[AgentType] = None,
+    ) -> Any:
+        """Crea un agente optimizado para un servicio GCP.
+        
+        Args:
+            service: Nombre del servicio GCP (ej: "bigquery", "storage")
+            agent_type: Tipo de agente (auto-selecciona si es None)
+            
+        Returns:
+            Agente especializado
+        """
+        # Auto-seleccionar tipo si no se especifica
+        if agent_type is None:
+            recommended = SERVICE_AGENT_MAPPING.get(
+                service.lower(),
+                [AgentType.RESEARCH],
+            )
+            agent_type = recommended[0]
+        
+        name = f"{service.lower()}-{agent_type.value}"
+        return self.create(name, agent_type, service)
+    
+    def create_workers(
+        self,
+        types: Optional[List[AgentType]] = None,
+    ) -> Dict[str, Any]:
+        """Crea conjunto de workers especializados.
+        
+        Args:
+            types: Lista de tipos a crear (default: todos basicos)
+            
+        Returns:
+            Diccionario de agentes por tipo
+        """
+        if types is None:
+            types = [
+                AgentType.RESEARCH,
+                AgentType.ANALYSIS,
+                AgentType.WRITER,
+                AgentType.CODE,
+            ]
+        
+        workers = {}
+        for agent_type in types:
+            name = f"worker-{agent_type.value}"
+            workers[agent_type.value] = self.create(name, agent_type)
+        
+        logger.info(f"Created {len(workers)} workers")
+        return workers
+    
+    def get(self, name: str) -> Optional[Any]:
+        """Obtiene un agente existente.
+        
+        Args:
+            name: Nombre del agente
+            
+        Returns:
+            Agente o None si no existe
+        """
+        return self._agents.get(name)
+    
+    def get_spec(self, name: str) -> Optional[AgentSpec]:
+        """Obtiene spec de un agente.
+        
+        Args:
+            name: Nombre del agente
+            
+        Returns:
+            AgentSpec o None
+        """
+        return self._specs.get(name)
+    
+    def list_agents(self) -> List[str]:
+        """Lista todos los agentes creados.
+        
+        Returns:
+            Lista de nombres de agentes
+        """
+        return list(self._agents.keys())
+    
+    def clear(self) -> None:
+        """Limpia todos los agentes cacheados."""
+        self._agents.clear()
+        self._specs.clear()
+        logger.info("Cleared all cached agents")
+
+
+# Instancia global para acceso facil
+_default_factory: Optional[AgentFactory] = None
+
+
+def get_factory() -> AgentFactory:
+    """Obtiene la fabrica global.
+    
+    Returns:
+        AgentFactory global
+    """
+    global _default_factory
+    if _default_factory is None:
+        _default_factory = AgentFactory()
+    return _default_factory
+
+
+def create_agent(
+    name: str,
+    agent_type: AgentType,
+    target_service: str = "",
+    **kwargs,
+) -> Any:
+    """Atajo para crear agentes usando la fabrica global.
+    
+    Args:
+        name: Nombre del agente
+        agent_type: Tipo de agente
+        target_service: Servicio objetivo
+        **kwargs: Args adicionales
+        
+        
+    Returns:
+        Agente creado
+    """
+    return get_factory().create(name, agent_type, target_service, **kwargs)
+
+
+def create_worker(agent_type: AgentType) -> Any:
+    """Atajo para crear un worker de tipo especifico.
+    
+    Args:
+        agent_type: Tipo de worker
+        
+    Returns:
+        Worker creado
+    """
+    return get_factory().create(f"worker-{agent_type.value}", agent_type)
+{%- endif %}
diff --git a/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/cloud/__init__.py b/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/cloud/__init__.py
new file mode 100644
index 0000000..ff9782a
--- /dev/null
+++ b/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/cloud/__init__.py
@@ -0,0 +1,21 @@
+{%- if cookiecutter.use_google_cloud == 'y' %}
+"""Google Cloud infrastructure modules.
+
+Este paquete contiene clientes y utilidades para interactuar
+con servicios de Google Cloud Platform.
+
+Modules:
+    firestore: Cliente Firestore para persistencia
+    run: Deployer para Cloud Run
+    pubsub: Cliente Pub/Sub para mensajeria
+"""
+from .firestore import FirestoreClient
+from .run import CloudRunDeployer
+from .pubsub import PubSubClient
+
+__all__ = [
+    "FirestoreClient",
+    "CloudRunDeployer",
+    "PubSubClient",
+]
+{%- endif %}
diff --git a/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/cloud/firestore.py b/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/cloud/firestore.py
new file mode 100644
index 0000000..7711f46
--- /dev/null
+++ b/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/cloud/firestore.py
@@ -0,0 +1,260 @@
+{%- if cookiecutter.use_google_cloud == 'y' %}
+"""Cliente Firestore para persistencia.
+
+Proporciona una interfaz async para Firestore con
+operaciones optimizadas para GENESIS.
+"""
+import logging
+import os
+from typing import Dict, Any, List, Optional
+
+logger = logging.getLogger(__name__)
+
+
+class FirestoreClient:
+    """Cliente async para Google Cloud Firestore.
+    
+    Proporciona operaciones CRUD con manejo automatico de
+    conexion y errores.
+    
+    Example:
+        >>> client = FirestoreClient()
+        >>> await client.set("collection/doc_id", {"key": "value"})
+        >>> doc = await client.get("collection/doc_id")
+        >>> print(doc)
+    """
+    
+    def __init__(self, project_id: Optional[str] = None):
+        """Inicializa cliente Firestore.
+        
+        Args:
+            project_id: ID del proyecto GCP. Si es None, usa
+                        GOOGLE_CLOUD_PROJECT o credenciales por defecto.
+        """
+        self._project_id = project_id or os.getenv("GOOGLE_CLOUD_PROJECT")
+        self._client = None
+        self._initialized = False
+    
+    @property
+    def client(self):
+        """Lazy initialization del cliente Firestore."""
+        if self._client is None:
+            try:
+                from google.cloud import firestore
+                
+                if self._project_id:
+                    self._client = firestore.AsyncClient(project=self._project_id)
+                else:
+                    self._client = firestore.AsyncClient()
+                
+                self._initialized = True
+                logger.info(f"Firestore client initialized for project: {self._project_id or 'default'}")
+                
+            except ImportError:
+                logger.error("google-cloud-firestore not installed")
+                raise ImportError(
+                    "Firestore requires google-cloud-firestore. "
+                    "Install with: pip install google-cloud-firestore"
+                )
+            except Exception as e:
+                logger.error(f"Failed to initialize Firestore: {e}")
+                raise
+        
+        return self._client
+    
+    async def get(self, path: str) -> Optional[Dict[str, Any]]:
+        """Obtiene un documento.
+        
+        Args:
+            path: Ruta del documento (collection/doc_id)
+            
+        Returns:
+            Datos del documento o None si no existe
+        """
+        try:
+            parts = path.split("/")
+            if len(parts) < 2:
+                raise ValueError(f"Invalid path: {path}")
+            
+            collection = parts[0]
+            doc_id = "/".join(parts[1:])
+            
+            doc_ref = self.client.collection(collection).document(doc_id)
+            doc = await doc_ref.get()
+            
+            if doc.exists:
+                return doc.to_dict()
+            return None
+            
+        except Exception as e:
+            logger.error(f"Failed to get document {path}: {e}")
+            raise
+    
+    async def set(self, path: str, data: Dict[str, Any], merge: bool = True) -> None:
+        """Guarda un documento.
+        
+        Args:
+            path: Ruta del documento (collection/doc_id)
+            data: Datos a guardar
+            merge: Si hacer merge con datos existentes
+        """
+        try:
+            parts = path.split("/")
+            if len(parts) < 2:
+                raise ValueError(f"Invalid path: {path}")
+            
+            collection = parts[0]
+            doc_id = "/".join(parts[1:])
+            
+            doc_ref = self.client.collection(collection).document(doc_id)
+            await doc_ref.set(data, merge=merge)
+            
+            logger.debug(f"Document saved: {path}")
+            
+        except Exception as e:
+            logger.error(f"Failed to set document {path}: {e}")
+            raise
+    
+    async def add(self, collection: str, data: Dict[str, Any]) -> str:
+        """Agrega un documento con ID auto-generado.
+        
+        Args:
+            collection: Nombre de la coleccion
+            data: Datos del documento
+            
+        Returns:
+            ID del documento creado
+        """
+        try:
+            from datetime import datetime
+            
+            # Agregar timestamp automatico
+            if "created_at" not in data:
+                data["created_at"] = datetime.utcnow().isoformat()
+            
+            doc_ref = self.client.collection(collection).document()
+            await doc_ref.set(data)
+            
+            logger.debug(f"Document added: {collection}/{doc_ref.id}")
+            return doc_ref.id
+            
+        except Exception as e:
+            logger.error(f"Failed to add document to {collection}: {e}")
+            raise
+    
+    async def delete(self, path: str) -> None:
+        """Elimina un documento.
+        
+        Args:
+            path: Ruta del documento
+        """
+        try:
+            parts = path.split("/")
+            if len(parts) < 2:
+                raise ValueError(f"Invalid path: {path}")
+            
+            collection = parts[0]
+            doc_id = "/".join(parts[1:])
+            
+            doc_ref = self.client.collection(collection).document(doc_id)
+            await doc_ref.delete()
+            
+            logger.debug(f"Document deleted: {path}")
+            
+        except Exception as e:
+            logger.error(f"Failed to delete document {path}: {e}")
+            raise
+    
+    async def query(
+        self,
+        collection: str,
+        filters: Optional[List[tuple]] = None,
+        order_by: Optional[str] = None,
+        order_direction: str = "asc",
+        limit: int = 100,
+    ) -> List[Dict[str, Any]]:
+        """Consulta documentos con filtros.
+        
+        Args:
+            collection: Nombre de la coleccion
+            filters: Lista de (field, operator, value)
+            order_by: Campo para ordenar
+            order_direction: "asc" o "desc"
+            limit: Numero maximo de resultados
+            
+        Returns:
+            Lista de documentos
+        """
+        try:
+            from google.cloud.firestore import Query
+            
+            query = self.client.collection(collection)
+            
+            # Aplicar filtros
+            if filters:
+                for field, op, value in filters:
+                    query = query.where(field, op, value)
+            
+            # Ordenar
+            if order_by:
+                direction = (
+                    Query.DESCENDING 
+                    if order_direction.lower() == "desc" 
+                    else Query.ASCENDING
+                )
+                query = query.order_by(order_by, direction=direction)
+            
+            # Limitar
+            query = query.limit(limit)
+            
+            # Ejecutar
+            docs = await query.get()
+            
+            return [
+                {"id": doc.id, **doc.to_dict()}
+                for doc in docs
+            ]
+            
+        except Exception as e:
+            logger.error(f"Failed to query {collection}: {e}")
+            raise
+    
+    async def list(self, collection: str, limit: int = 1000) -> List[Dict[str, Any]]:
+        """Lista todos los documentos de una coleccion.
+        
+        Args:
+            collection: Nombre de la coleccion
+            limit: Numero maximo de documentos
+            
+        Returns:
+            Lista de documentos
+        """
+        return await self.query(collection, limit=limit)
+    
+    async def get_genesis_state(self) -> Dict[str, Any]:
+        """Obtiene estado actual de GENESIS desde Firestore.
+        
+        Returns:
+            Estado del sistema
+        """
+        state = await self.get("genesis_state/current")
+        return state or {}
+    
+    async def update_genesis_state(self, state: Dict[str, Any]) -> None:
+        """Actualiza estado de GENESIS en Firestore.
+        
+        Args:
+            state: Nuevo estado
+        """
+        from datetime import datetime
+        state["updated_at"] = datetime.utcnow().isoformat()
+        await self.set("genesis_state/current", state)
+    
+    async def close(self) -> None:
+        """Cierra la conexion."""
+        if self._client:
+            # Note: AsyncClient no tiene metodo close explicito
+            # pero es buena practica tenerlo para consistencia
+            self._client = None
+            self._initialized = False
+{%- endif %}
diff --git a/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/cloud/pubsub.py b/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/cloud/pubsub.py
new file mode 100644
index 0000000..971c283
--- /dev/null
+++ b/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/cloud/pubsub.py
@@ -0,0 +1,433 @@
+{%- if cookiecutter.use_google_cloud == 'y' %}
+"""Cliente Pub/Sub para mensajeria.
+
+Proporciona una interfaz async para Google Cloud Pub/Sub
+optimizada para comunicacion entre componentes de GENESIS.
+"""
+import json
+import logging
+import os
+from dataclasses import dataclass
+from datetime import datetime
+from typing import Callable, Dict, Any, List, Optional
+
+logger = logging.getLogger(__name__)
+
+
+@dataclass
+class PubSubMessage:
+    """Mensaje de Pub/Sub.
+    
+    Attributes:
+        data: Datos del mensaje (dict serializado)
+        attributes: Atributos/metadata del mensaje
+        message_id: ID del mensaje (asignado por Pub/Sub)
+        publish_time: Momento de publicacion
+    """
+    data: Dict[str, Any]
+    attributes: Dict[str, str] = None
+    message_id: Optional[str] = None
+    publish_time: Optional[datetime] = None
+    
+    def __post_init__(self):
+        if self.attributes is None:
+            self.attributes = {}
+    
+    def to_bytes(self) -> bytes:
+        """Serializa el mensaje a bytes."""
+        return json.dumps(self.data).encode("utf-8")
+    
+    @classmethod
+    def from_bytes(cls, data: bytes, **kwargs) -> "PubSubMessage":
+        """Deserializa mensaje desde bytes."""
+        return cls(data=json.loads(data.decode("utf-8")), **kwargs)
+
+
+class PubSubClient:
+    """Cliente async para Google Cloud Pub/Sub.
+    
+    Proporciona publicacion y suscripcion a topics
+    con serialization automatica de mensajes.
+    
+    Example:
+        >>> client = PubSubClient()
+        >>> await client.publish("my-topic", {"event": "test"})
+        >>> 
+        >>> async for msg in client.subscribe("my-subscription"):
+        ...     print(msg.data)
+        ...     await msg.ack()
+    """
+    
+    # Prefijo para topics de GENESIS
+    GENESIS_PREFIX = "genesis-"
+    
+    def __init__(self, project_id: Optional[str] = None):
+        """Inicializa cliente Pub/Sub.
+        
+        Args:
+            project_id: ID del proyecto GCP
+        """
+        self._project_id = project_id or os.getenv("GOOGLE_CLOUD_PROJECT")
+        self._publisher = None
+        self._subscriber = None
+    
+    @property
+    def publisher(self):
+        """Lazy initialization del publisher."""
+        if self._publisher is None:
+            try:
+                from google.cloud import pubsub_v1
+                self._publisher = pubsub_v1.PublisherClient()
+                logger.info("Pub/Sub publisher initialized")
+            except ImportError:
+                raise ImportError(
+                    "Pub/Sub requires google-cloud-pubsub. "
+                    "Install with: pip install google-cloud-pubsub"
+                )
+        return self._publisher
+    
+    @property
+    def subscriber(self):
+        """Lazy initialization del subscriber."""
+        if self._subscriber is None:
+            try:
+                from google.cloud import pubsub_v1
+                self._subscriber = pubsub_v1.SubscriberClient()
+                logger.info("Pub/Sub subscriber initialized")
+            except ImportError:
+                raise ImportError(
+                    "Pub/Sub requires google-cloud-pubsub. "
+                    "Install with: pip install google-cloud-pubsub"
+                )
+        return self._subscriber
+    
+    def _topic_path(self, topic: str) -> str:
+        """Construye path completo del topic.
+        
+        Args:
+            topic: Nombre del topic
+            
+        Returns:
+            Path completo del topic
+        """
+        return f"projects/{self._project_id}/topics/{topic}"
+    
+    def _subscription_path(self, subscription: str) -> str:
+        """Construye path completo de la suscripcion.
+        
+        Args:
+            subscription: Nombre de la suscripcion
+            
+        Returns:
+            Path completo
+        """
+        return f"projects/{self._project_id}/subscriptions/{subscription}"
+    
+    async def publish(
+        self,
+        topic: str,
+        data: Dict[str, Any],
+        attributes: Optional[Dict[str, str]] = None,
+    ) -> str:
+        """Publica un mensaje a un topic.
+        
+        Args:
+            topic: Nombre del topic
+            data: Datos del mensaje
+            attributes: Atributos opcionales
+            
+        Returns:
+            ID del mensaje publicado
+        """
+        import asyncio
+        
+        message = PubSubMessage(data=data, attributes=attributes or {})
+        
+        # Agregar atributos por defecto
+        message.attributes.setdefault("source", "genesis")
+        message.attributes.setdefault("timestamp", datetime.utcnow().isoformat())
+        
+        topic_path = self._topic_path(topic)
+        
+        # Publicar (sync wrapper ya que publisher no es async nativo)
+        future = self.publisher.publish(
+            topic_path,
+            message.to_bytes(),
+            **message.attributes,
+        )
+        
+        # Esperar resultado
+        message_id = await asyncio.get_event_loop().run_in_executor(
+            None, future.result
+        )
+        
+        logger.debug(f"Published message {message_id} to {topic}")
+        return message_id
+    
+    async def publish_batch(
+        self,
+        topic: str,
+        messages: List[Dict[str, Any]],
+    ) -> List[str]:
+        """Publica multiples mensajes a un topic.
+        
+        Args:
+            topic: Nombre del topic
+            messages: Lista de datos a publicar
+            
+        Returns:
+            Lista de IDs de mensajes publicados
+        """
+        import asyncio
+        
+        topic_path = self._topic_path(topic)
+        futures = []
+        
+        for data in messages:
+            message = PubSubMessage(data=data)
+            message.attributes["source"] = "genesis"
+            message.attributes["timestamp"] = datetime.utcnow().isoformat()
+            
+            future = self.publisher.publish(
+                topic_path,
+                message.to_bytes(),
+                **message.attributes,
+            )
+            futures.append(future)
+        
+        # Esperar todos
+        loop = asyncio.get_event_loop()
+        message_ids = await asyncio.gather(*[
+            loop.run_in_executor(None, f.result)
+            for f in futures
+        ])
+        
+        logger.debug(f"Published {len(message_ids)} messages to {topic}")
+        return list(message_ids)
+    
+    async def subscribe(
+        self,
+        subscription: str,
+        callback: Callable[[PubSubMessage], None],
+        max_messages: int = 10,
+        timeout: float = 30.0,
+    ) -> None:
+        """Suscribe a una subscription con callback.
+        
+        Args:
+            subscription: Nombre de la suscripcion
+            callback: Funcion a llamar por mensaje
+            max_messages: Mensajes maximos en paralelo
+            timeout: Timeout en segundos (None = indefinido)
+        """
+        import asyncio
+        
+        subscription_path = self._subscription_path(subscription)
+        
+        def message_callback(message):
+            try:
+                msg = PubSubMessage(
+                    data=json.loads(message.data.decode("utf-8")),
+                    attributes=dict(message.attributes),
+                    message_id=message.message_id,
+                    publish_time=message.publish_time,
+                )
+                callback(msg)
+                message.ack()
+            except Exception as e:
+                logger.error(f"Error processing message: {e}")
+                message.nack()
+        
+        streaming_pull_future = self.subscriber.subscribe(
+            subscription_path,
+            callback=message_callback,
+        )
+        
+        logger.info(f"Subscribed to {subscription}")
+        
+        try:
+            if timeout:
+                await asyncio.wait_for(
+                    asyncio.get_event_loop().run_in_executor(
+                        None, streaming_pull_future.result
+                    ),
+                    timeout=timeout,
+                )
+            else:
+                await asyncio.get_event_loop().run_in_executor(
+                    None, streaming_pull_future.result
+                )
+        except asyncio.TimeoutError:
+            streaming_pull_future.cancel()
+            logger.info(f"Subscription timeout after {timeout}s")
+    
+    async def pull(
+        self,
+        subscription: str,
+        max_messages: int = 10,
+        timeout: float = 5.0,
+    ) -> List[PubSubMessage]:
+        """Pull mensajes de una suscripcion.
+        
+        Args:
+            subscription: Nombre de la suscripcion
+            max_messages: Numero maximo de mensajes
+            timeout: Timeout de pull
+            
+        Returns:
+            Lista de mensajes
+        """
+        import asyncio
+        
+        subscription_path = self._subscription_path(subscription)
+        
+        response = await asyncio.get_event_loop().run_in_executor(
+            None,
+            lambda: self.subscriber.pull(
+                subscription=subscription_path,
+                max_messages=max_messages,
+            ),
+        )
+        
+        messages = []
+        ack_ids = []
+        
+        for received_message in response.received_messages:
+            msg = PubSubMessage(
+                data=json.loads(received_message.message.data.decode("utf-8")),
+                attributes=dict(received_message.message.attributes),
+                message_id=received_message.message.message_id,
+                publish_time=received_message.message.publish_time,
+            )
+            messages.append(msg)
+            ack_ids.append(received_message.ack_id)
+        
+        # Ack todos los mensajes
+        if ack_ids:
+            self.subscriber.acknowledge(
+                subscription=subscription_path,
+                ack_ids=ack_ids,
+            )
+        
+        return messages
+    
+    async def create_topic(self, topic: str) -> str:
+        """Crea un topic.
+        
+        Args:
+            topic: Nombre del topic
+            
+        Returns:
+            Path completo del topic creado
+        """
+        import asyncio
+        
+        topic_path = self._topic_path(topic)
+        
+        try:
+            await asyncio.get_event_loop().run_in_executor(
+                None,
+                lambda: self.publisher.create_topic(name=topic_path),
+            )
+            logger.info(f"Created topic: {topic}")
+        except Exception as e:
+            if "already exists" in str(e).lower():
+                logger.debug(f"Topic already exists: {topic}")
+            else:
+                raise
+        
+        return topic_path
+    
+    async def create_subscription(
+        self,
+        subscription: str,
+        topic: str,
+        ack_deadline_seconds: int = 60,
+    ) -> str:
+        """Crea una suscripcion.
+        
+        Args:
+            subscription: Nombre de la suscripcion
+            topic: Topic al que suscribir
+            ack_deadline_seconds: Deadline de ack
+            
+        Returns:
+            Path completo de la suscripcion
+        """
+        import asyncio
+        
+        subscription_path = self._subscription_path(subscription)
+        topic_path = self._topic_path(topic)
+        
+        try:
+            await asyncio.get_event_loop().run_in_executor(
+                None,
+                lambda: self.subscriber.create_subscription(
+                    name=subscription_path,
+                    topic=topic_path,
+                    ack_deadline_seconds=ack_deadline_seconds,
+                ),
+            )
+            logger.info(f"Created subscription: {subscription}")
+        except Exception as e:
+            if "already exists" in str(e).lower():
+                logger.debug(f"Subscription already exists: {subscription}")
+            else:
+                raise
+        
+        return subscription_path
+    
+    # =================================================================
+    # Topics predefinidos para GENESIS
+    # =================================================================
+    
+    async def publish_cycle_event(
+        self,
+        cycle_id: str,
+        event_type: str,
+        data: Dict[str, Any],
+    ) -> str:
+        """Publica evento de ciclo GENESIS.
+        
+        Args:
+            cycle_id: ID del ciclo
+            event_type: Tipo de evento (started, completed, error)
+            data: Datos del evento
+            
+        Returns:
+            ID del mensaje
+        """
+        return await self.publish(
+            f"{self.GENESIS_PREFIX}cycles",
+            {
+                "cycle_id": cycle_id,
+                "event_type": event_type,
+                **data,
+            },
+        )
+    
+    async def publish_agent_event(
+        self,
+        agent_name: str,
+        event_type: str,
+        data: Dict[str, Any],
+    ) -> str:
+        """Publica evento de agente.
+        
+        Args:
+            agent_name: Nombre del agente
+            event_type: Tipo de evento
+            data: Datos del evento
+            
+        Returns:
+            ID del mensaje
+        """
+        return await self.publish(
+            f"{self.GENESIS_PREFIX}agents",
+            {
+                "agent_name": agent_name,
+                "event_type": event_type,
+                **data,
+            },
+        )
+{%- endif %}
diff --git a/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/cloud/run.py b/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/cloud/run.py
new file mode 100644
index 0000000..eeed6e9
--- /dev/null
+++ b/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/cloud/run.py
@@ -0,0 +1,352 @@
+{%- if cookiecutter.use_google_cloud == 'y' %}
+"""Cloud Run deployment operations.
+
+Proporciona utilidades para desplegar y gestionar
+servicios en Google Cloud Run.
+"""
+import logging
+import os
+import subprocess
+from dataclasses import dataclass
+from typing import Optional, Dict, Any, List
+
+logger = logging.getLogger(__name__)
+
+
+@dataclass
+class DeploymentConfig:
+    """Configuracion de deployment.
+    
+    Attributes:
+        service_name: Nombre del servicio Cloud Run
+        region: Region de deployment
+        image: Imagen de contenedor
+        memory: Memoria asignada
+        cpu: CPUs asignados
+        min_instances: Instancias minimas
+        max_instances: Instancias maximas
+        env_vars: Variables de entorno
+        allow_unauthenticated: Si permite acceso publico
+    """
+    service_name: str = "genesis"
+    region: str = "us-central1"
+    image: Optional[str] = None
+    memory: str = "256Mi"
+    cpu: str = "1"
+    min_instances: int = 0
+    max_instances: int = 1
+    env_vars: Dict[str, str] = None
+    allow_unauthenticated: bool = False
+    
+    def __post_init__(self):
+        if self.env_vars is None:
+            self.env_vars = {}
+
+
+class CloudRunDeployer:
+    """Deployer para Google Cloud Run.
+    
+    Gestiona el deployment de servicios usando gcloud CLI
+    o la API de Cloud Run directamente.
+    
+    Example:
+        >>> deployer = CloudRunDeployer()
+        >>> url = await deployer.deploy()
+        >>> print(f"Service deployed at: {url}")
+    """
+    
+    def __init__(
+        self,
+        project_id: Optional[str] = None,
+        config: Optional[DeploymentConfig] = None,
+    ):
+        """Inicializa el deployer.
+        
+        Args:
+            project_id: ID del proyecto GCP
+            config: Configuracion de deployment
+        """
+        self._project_id = project_id or os.getenv("GOOGLE_CLOUD_PROJECT")
+        self._config = config or DeploymentConfig()
+        self._client = None
+    
+    @property
+    def config(self) -> DeploymentConfig:
+        """Retorna configuracion actual."""
+        return self._config
+    
+    async def deploy(
+        self,
+        image: Optional[str] = None,
+        env_vars: Optional[Dict[str, str]] = None,
+    ) -> str:
+        """Despliega servicio a Cloud Run.
+        
+        Args:
+            image: Imagen de contenedor (override config)
+            env_vars: Variables de entorno adicionales
+            
+        Returns:
+            URL del servicio desplegado
+        """
+        logger.info(f"[CLOUDRUN] Deploying {self._config.service_name}...")
+        
+        # Usar imagen del config o la proporcionada
+        deploy_image = image or self._config.image
+        if not deploy_image:
+            raise ValueError("No image specified for deployment")
+        
+        # Combinar env vars
+        all_env_vars = {**self._config.env_vars}
+        if env_vars:
+            all_env_vars.update(env_vars)
+        
+        # Intentar usar API primero, fallback a gcloud
+        try:
+            url = await self._deploy_via_api(deploy_image, all_env_vars)
+        except ImportError:
+            logger.info("[CLOUDRUN] API not available, using gcloud CLI")
+            url = await self._deploy_via_gcloud(deploy_image, all_env_vars)
+        
+        logger.info(f"[CLOUDRUN] Deployed successfully: {url}")
+        return url
+    
+    async def _deploy_via_api(
+        self,
+        image: str,
+        env_vars: Dict[str, str],
+    ) -> str:
+        """Despliega usando la API de Cloud Run.
+        
+        Args:
+            image: Imagen de contenedor
+            env_vars: Variables de entorno
+            
+        Returns:
+            URL del servicio
+        """
+        from google.cloud import run_v2
+        
+        client = run_v2.ServicesAsyncClient()
+        
+        # Construir especificacion del servicio
+        service = run_v2.Service(
+            template=run_v2.RevisionTemplate(
+                containers=[
+                    run_v2.Container(
+                        image=image,
+                        resources=run_v2.ResourceRequirements(
+                            limits={
+                                "memory": self._config.memory,
+                                "cpu": self._config.cpu,
+                            },
+                        ),
+                        env=[
+                            run_v2.EnvVar(name=k, value=v)
+                            for k, v in env_vars.items()
+                        ],
+                    ),
+                ],
+                scaling=run_v2.RevisionScaling(
+                    min_instance_count=self._config.min_instances,
+                    max_instance_count=self._config.max_instances,
+                ),
+            ),
+        )
+        
+        parent = f"projects/{self._project_id}/locations/{self._config.region}"
+        
+        # Crear o actualizar servicio
+        operation = await client.create_service(
+            parent=parent,
+            service=service,
+            service_id=self._config.service_name,
+        )
+        
+        result = await operation.result()
+        return result.uri
+    
+    async def _deploy_via_gcloud(
+        self,
+        image: str,
+        env_vars: Dict[str, str],
+    ) -> str:
+        """Despliega usando gcloud CLI.
+        
+        Args:
+            image: Imagen de contenedor
+            env_vars: Variables de entorno
+            
+        Returns:
+            URL del servicio
+        """
+        import asyncio
+        
+        # Construir comando
+        cmd = [
+            "gcloud", "run", "deploy", self._config.service_name,
+            "--image", image,
+            "--region", self._config.region,
+            "--memory", self._config.memory,
+            "--cpu", self._config.cpu,
+            "--min-instances", str(self._config.min_instances),
+            "--max-instances", str(self._config.max_instances),
+            "--quiet",
+        ]
+        
+        # Agregar proyecto si esta definido
+        if self._project_id:
+            cmd.extend(["--project", self._project_id])
+        
+        # Agregar env vars
+        if env_vars:
+            env_str = ",".join(f"{k}={v}" for k, v in env_vars.items())
+            cmd.extend(["--set-env-vars", env_str])
+        
+        # Agregar flag de acceso
+        if self._config.allow_unauthenticated:
+            cmd.append("--allow-unauthenticated")
+        else:
+            cmd.append("--no-allow-unauthenticated")
+        
+        # Ejecutar comando
+        logger.debug(f"[CLOUDRUN] Running: {' '.join(cmd)}")
+        
+        process = await asyncio.create_subprocess_exec(
+            *cmd,
+            stdout=subprocess.PIPE,
+            stderr=subprocess.PIPE,
+        )
+        
+        stdout, stderr = await process.communicate()
+        
+        if process.returncode != 0:
+            raise RuntimeError(
+                f"gcloud deploy failed: {stderr.decode()}"
+            )
+        
+        # Obtener URL del servicio
+        return await self.get_service_url()
+    
+    async def get_service_url(self) -> str:
+        """Obtiene URL del servicio desplegado.
+        
+        Returns:
+            URL del servicio
+        """
+        import asyncio
+        
+        cmd = [
+            "gcloud", "run", "services", "describe",
+            self._config.service_name,
+            "--region", self._config.region,
+            "--format", "value(status.url)",
+        ]
+        
+        if self._project_id:
+            cmd.extend(["--project", self._project_id])
+        
+        process = await asyncio.create_subprocess_exec(
+            *cmd,
+            stdout=subprocess.PIPE,
+            stderr=subprocess.PIPE,
+        )
+        
+        stdout, stderr = await process.communicate()
+        
+        if process.returncode != 0:
+            raise RuntimeError(f"Failed to get service URL: {stderr.decode()}")
+        
+        return stdout.decode().strip()
+    
+    async def get_service_status(self) -> Dict[str, Any]:
+        """Obtiene estado del servicio.
+        
+        Returns:
+            Informacion del servicio
+        """
+        import asyncio
+        import json
+        
+        cmd = [
+            "gcloud", "run", "services", "describe",
+            self._config.service_name,
+            "--region", self._config.region,
+            "--format", "json",
+        ]
+        
+        if self._project_id:
+            cmd.extend(["--project", self._project_id])
+        
+        process = await asyncio.create_subprocess_exec(
+            *cmd,
+            stdout=subprocess.PIPE,
+            stderr=subprocess.PIPE,
+        )
+        
+        stdout, stderr = await process.communicate()
+        
+        if process.returncode != 0:
+            raise RuntimeError(f"Failed to get service status: {stderr.decode()}")
+        
+        return json.loads(stdout.decode())
+    
+    async def list_revisions(self) -> List[Dict[str, Any]]:
+        """Lista revisiones del servicio.
+        
+        Returns:
+            Lista de revisiones
+        """
+        import asyncio
+        import json
+        
+        cmd = [
+            "gcloud", "run", "revisions", "list",
+            "--service", self._config.service_name,
+            "--region", self._config.region,
+            "--format", "json",
+        ]
+        
+        if self._project_id:
+            cmd.extend(["--project", self._project_id])
+        
+        process = await asyncio.create_subprocess_exec(
+            *cmd,
+            stdout=subprocess.PIPE,
+            stderr=subprocess.PIPE,
+        )
+        
+        stdout, stderr = await process.communicate()
+        
+        if process.returncode != 0:
+            return []
+        
+        return json.loads(stdout.decode())
+    
+    async def delete_service(self) -> bool:
+        """Elimina el servicio.
+        
+        Returns:
+            True si se elimino exitosamente
+        """
+        import asyncio
+        
+        cmd = [
+            "gcloud", "run", "services", "delete",
+            self._config.service_name,
+            "--region", self._config.region,
+            "--quiet",
+        ]
+        
+        if self._project_id:
+            cmd.extend(["--project", self._project_id])
+        
+        process = await asyncio.create_subprocess_exec(
+            *cmd,
+            stdout=subprocess.PIPE,
+            stderr=subprocess.PIPE,
+        )
+        
+        await process.communicate()
+        return process.returncode == 0
+{%- endif %}
diff --git a/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/genesis/__init__.py b/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/genesis/__init__.py
new file mode 100644
index 0000000..e4cecca
--- /dev/null
+++ b/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/genesis/__init__.py
@@ -0,0 +1,45 @@
+{%- if cookiecutter.use_google_adk == 'y' and cookiecutter.use_google_cloud == 'y' %}
+"""GENESIS - Sistema Autopoietico en Google Cloud.
+
+GENESIS es un sistema de IA auto-programable que:
+- PERCIBE su entorno GCP automaticamente
+- PIENSA usando Gemini para razonar
+- ACTUA generando y ejecutando codigo
+- EVOLUCIONA mejorando su propio codigo
+- PERSISTE su estado en Firestore
+- VIVE en Cloud Run
+
+Arquitectura:
+    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
+    â”‚  PERCEIVE â†’ THINK â†’ ACT â†’ REMEMBER      â”‚
+    â”‚       â†‘                      â”‚          â”‚
+    â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€ EVOLVE â”€â”€â”€â”€â”€â”€â”˜          â”‚
+    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
+
+Uso:
+    >>> from {{cookiecutter.package_name}}.genesis import GenesisCore
+    >>> genesis = GenesisCore()
+    >>> result = await genesis.run_cycle()
+    >>> print(result.success)
+"""
+from .core import GenesisCore, CycleResult
+from .perceive import PerceiveModule, EnvironmentContext
+from .think import ThinkModule, ActionPlan, Action
+from .act import ActModule, ActionResult
+from .memory import MemoryModule
+from .evolve import EvolveModule
+
+__all__ = [
+    "GenesisCore",
+    "CycleResult",
+    "PerceiveModule",
+    "EnvironmentContext",
+    "ThinkModule",
+    "ActionPlan",
+    "Action",
+    "ActModule",
+    "ActionResult",
+    "MemoryModule",
+    "EvolveModule",
+]
+{%- endif %}
diff --git a/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/genesis/act.py b/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/genesis/act.py
new file mode 100644
index 0000000..23005b2
--- /dev/null
+++ b/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/genesis/act.py
@@ -0,0 +1,471 @@
+{%- if cookiecutter.use_google_adk == 'y' and cookiecutter.use_google_cloud == 'y' %}
+"""Modulo de Accion - Ejecuta planes y genera codigo.
+
+Este modulo implementa la fase ACT del ciclo GENESIS.
+Ejecuta las acciones planificadas por el modulo Think,
+incluyendo generacion de codigo, deployment y queries.
+"""
+import ast
+import logging
+import os
+from dataclasses import dataclass, field
+from typing import List, Any, Optional, Dict
+from datetime import datetime
+
+logger = logging.getLogger(__name__)
+
+
+@dataclass
+class ActionResult:
+    """Resultado de ejecutar acciones.
+    
+    Attributes:
+        actions: Lista de acciones ejecutadas (tipo:target)
+        success: Si todas las acciones fueron exitosas
+        outputs: Outputs de cada accion
+        errors: Errores encontrados
+        timestamp: Momento de ejecucion
+    """
+    actions: List[str] = field(default_factory=list)
+    success: bool = True
+    outputs: List[Any] = field(default_factory=list)
+    errors: List[str] = field(default_factory=list)
+    timestamp: datetime = field(default_factory=datetime.utcnow)
+    
+    def to_dict(self) -> dict:
+        """Convierte a diccionario."""
+        return {
+            "actions": self.actions,
+            "success": self.success,
+            "outputs": [str(o)[:200] for o in self.outputs],  # Truncar outputs largos
+            "errors": self.errors,
+            "timestamp": self.timestamp.isoformat(),
+        }
+    
+    @classmethod
+    def empty(cls) -> "ActionResult":
+        """Crea resultado vacio para casos de error."""
+        return cls(
+            actions=[],
+            success=False,
+            errors=["No actions executed due to error"],
+        )
+
+
+class ActModule:
+    """Modulo de ejecucion de acciones.
+    
+    Ejecuta las acciones del plan generado por ThinkModule.
+    Soporta generacion de agentes, plugins, deployment y queries.
+    
+    Example:
+        >>> act = ActModule()
+        >>> act.think = think_module  # Inyectar dependencia
+        >>> result = await act.execute(plan)
+        >>> print(f"Success: {result.success}")
+    """
+    
+    # Directorio base para codigo generado
+    GENERATED_CODE_DIR = "generated"
+    
+    def __init__(self):
+        """Inicializa el modulo de accion."""
+        self.think = None  # Inyectado por GenesisCore
+        self._generated_files: List[str] = []
+        logger.info("ActModule initialized")
+    
+    async def execute(self, plan) -> ActionResult:
+        """Ejecuta todas las acciones del plan.
+        
+        Args:
+            plan: ActionPlan con acciones a ejecutar
+            
+        Returns:
+            ActionResult con resultados
+        """
+        logger.info(f"[ACT] Executing plan with {len(plan.actions)} actions...")
+        
+        outputs: List[Any] = []
+        errors: List[str] = []
+        actions_done: List[str] = []
+        
+        # Ordenar por prioridad (mayor primero)
+        sorted_actions = plan.get_actions_by_priority()
+        
+        for action in sorted_actions:
+            action_id = f"{action.type}:{action.target}"
+            logger.info(f"[ACT] Executing: {action_id}")
+            
+            try:
+                result = await self._execute_action(action)
+                outputs.append(result)
+                actions_done.append(action_id)
+                logger.info(f"[ACT] Success: {action_id}")
+                
+            except Exception as e:
+                error_msg = f"{action_id} - {str(e)}"
+                errors.append(error_msg)
+                logger.error(f"[ACT] Failed: {error_msg}")
+        
+        success = len(errors) == 0 and len(actions_done) > 0
+        
+        logger.info(
+            f"[ACT] Execution complete: "
+            f"{len(actions_done)} succeeded, "
+            f"{len(errors)} failed"
+        )
+        
+        return ActionResult(
+            actions=actions_done,
+            success=success,
+            outputs=outputs,
+            errors=errors,
+        )
+    
+    async def _execute_action(self, action) -> Any:
+        """Ejecuta una accion individual.
+        
+        Args:
+            action: Action a ejecutar
+            
+        Returns:
+            Resultado de la accion
+            
+        Raises:
+            ValueError: Si el tipo de accion es desconocido
+        """
+        handlers = {
+            "generate_agent": self._generate_agent,
+            "generate_plugin": self._generate_plugin,
+            "deploy": self._deploy,
+            "query": self._query,
+            "modify_code": self._modify_code,
+        }
+        
+        handler = handlers.get(action.type)
+        if not handler:
+            raise ValueError(f"Unknown action type: {action.type}")
+        
+        return await handler(action)
+    
+    async def _generate_agent(self, action) -> Dict[str, Any]:
+        """Genera un nuevo agente para un servicio GCP.
+        
+        Args:
+            action: Action con spec del agente
+            
+        Returns:
+            Info sobre el agente generado
+        """
+        if self.think is None:
+            raise RuntimeError("ThinkModule not injected")
+        
+        target = action.target
+        spec = action.spec
+        
+        # Construir especificacion completa
+        agent_spec = f'''Agente especializado para el servicio GCP: {target}
+
+Descripcion: {spec.get("description", f"Agente para interactuar con {target}")}
+
+Requerimientos:
+{chr(10).join(f"- {r}" for r in spec.get("requirements", []))}
+
+El agente debe:
+1. Heredar de GoogleADKAgent
+2. Tener system prompt especializado para {target}
+3. Implementar metodos para las operaciones principales de {target}
+4. Manejar errores de API de Google Cloud
+5. Incluir logging apropiado
+6. Ser compatible con async/await
+
+Nombre de la clase: {self._to_class_name(target)}Agent
+'''
+        
+        # Generar codigo
+        code = await self.think.generate_code(agent_spec)
+        
+        # Validar sintaxis
+        ast.parse(code)
+        
+        # Guardar archivo
+        filename = f"{target.lower().replace('.', '_')}_agent.py"
+        filepath = await self._save_generated_code(
+            f"agents/{filename}",
+            code,
+            f"Generated agent for {target}"
+        )
+        
+        return {
+            "type": "agent",
+            "target": target,
+            "class_name": f"{self._to_class_name(target)}Agent",
+            "filepath": filepath,
+            "code_length": len(code),
+        }
+    
+    async def _generate_plugin(self, action) -> Dict[str, Any]:
+        """Genera un nuevo plugin de discovery.
+        
+        Args:
+            action: Action con spec del plugin
+            
+        Returns:
+            Info sobre el plugin generado
+        """
+        if self.think is None:
+            raise RuntimeError("ThinkModule not injected")
+        
+        target = action.target
+        spec = action.spec
+        
+        # Construir especificacion
+        plugin_spec = f'''Plugin de discovery para el servicio GCP: {target}
+
+Descripcion: {spec.get("description", f"Plugin para descubrir recursos de {target}")}
+
+El plugin debe:
+1. Heredar de BaseGCPPlugin
+2. Implementar service_patterns con patrones para identificar {target}
+3. Implementar required_packages con los paquetes necesarios
+4. Implementar discover_resources() que:
+   - Conecta al servicio usando google-cloud-{target.lower()}
+   - Lista los recursos disponibles
+   - Retorna dict con type, count y resources
+
+Nombre de la clase: {self._to_class_name(target)}Plugin
+'''
+        
+        # Generar codigo
+        code = await self.think.generate_code(plugin_spec)
+        
+        # Validar sintaxis
+        ast.parse(code)
+        
+        # Guardar archivo
+        filename = f"{target.lower().replace('.', '_')}_plugin.py"
+        filepath = await self._save_generated_code(
+            f"plugins/{filename}",
+            code,
+            f"Generated plugin for {target}"
+        )
+        
+        return {
+            "type": "plugin",
+            "target": target,
+            "class_name": f"{self._to_class_name(target)}Plugin",
+            "filepath": filepath,
+            "code_length": len(code),
+        }
+    
+    async def _deploy(self, action) -> Dict[str, Any]:
+        """Despliega cambios a Cloud Run.
+        
+        Args:
+            action: Action con spec del deployment
+            
+        Returns:
+            Info sobre el deployment
+        """
+        logger.info(f"[ACT] Deploying: {action.target}")
+        
+        try:
+            from ..cloud.run import CloudRunDeployer
+            deployer = CloudRunDeployer()
+            url = await deployer.deploy()
+            
+            return {
+                "type": "deploy",
+                "target": action.target,
+                "url": url,
+                "status": "deployed",
+            }
+        except ImportError:
+            logger.warning("[ACT] CloudRunDeployer not available")
+            return {
+                "type": "deploy",
+                "target": action.target,
+                "status": "skipped",
+                "reason": "CloudRunDeployer not installed",
+            }
+        except Exception as e:
+            return {
+                "type": "deploy",
+                "target": action.target,
+                "status": "failed",
+                "error": str(e),
+            }
+    
+    async def _query(self, action) -> Dict[str, Any]:
+        """Ejecuta una consulta.
+        
+        Args:
+            action: Action con spec de la query
+            
+        Returns:
+            Resultados de la query
+        """
+        target = action.target
+        spec = action.spec
+        
+        # Por ahora, queries se delegan a los agentes especializados
+        # o se implementan segun el tipo de query
+        
+        query_type = spec.get("query_type", "info")
+        
+        if query_type == "info":
+            return {
+                "type": "query",
+                "target": target,
+                "query_type": query_type,
+                "result": f"Query info for {target} - pending implementation",
+            }
+        
+        return {
+            "type": "query",
+            "target": target,
+            "query_type": query_type,
+            "status": "not_implemented",
+        }
+    
+    async def _modify_code(self, action) -> Dict[str, Any]:
+        """Modifica codigo existente.
+        
+        Args:
+            action: Action con spec de la modificacion
+            
+        Returns:
+            Info sobre la modificacion
+        """
+        if self.think is None:
+            raise RuntimeError("ThinkModule not injected")
+        
+        target = action.target
+        spec = action.spec
+        
+        filepath = spec.get("filepath", "")
+        modification = spec.get("modification", "")
+        
+        if not filepath or not modification:
+            return {
+                "type": "modify_code",
+                "target": target,
+                "status": "skipped",
+                "reason": "Missing filepath or modification spec",
+            }
+        
+        # Leer codigo actual
+        try:
+            with open(filepath, "r") as f:
+                current_code = f.read()
+        except FileNotFoundError:
+            return {
+                "type": "modify_code",
+                "target": target,
+                "status": "failed",
+                "error": f"File not found: {filepath}",
+            }
+        
+        # Generar modificacion
+        mod_spec = f'''Modifica el siguiente codigo Python:
+
+```python
+{current_code}
+```
+
+Modificacion requerida: {modification}
+
+Retorna el codigo completo modificado.
+'''
+        
+        new_code = await self.think.generate_code(mod_spec)
+        
+        # Validar sintaxis
+        ast.parse(new_code)
+        
+        # Guardar backup y nuevo codigo
+        backup_path = f"{filepath}.backup"
+        with open(backup_path, "w") as f:
+            f.write(current_code)
+        
+        with open(filepath, "w") as f:
+            f.write(new_code)
+        
+        return {
+            "type": "modify_code",
+            "target": target,
+            "filepath": filepath,
+            "backup": backup_path,
+            "status": "modified",
+        }
+    
+    async def _save_generated_code(
+        self,
+        relative_path: str,
+        code: str,
+        description: str,
+    ) -> str:
+        """Guarda codigo generado.
+        
+        Args:
+            relative_path: Ruta relativa dentro de generated/
+            code: Codigo a guardar
+            description: Descripcion para el header
+            
+        Returns:
+            Ruta completa del archivo guardado
+        """
+        # Header con metadata
+        header = f'''"""Auto-generated by GENESIS.
+
+{description}
+Generated at: {datetime.utcnow().isoformat()}
+
+DO NOT EDIT MANUALLY - This file is managed by GENESIS.
+"""
+'''
+        
+        full_code = header + "\n" + code
+        
+        # Crear directorio si no existe
+        base_dir = os.path.join(os.path.dirname(__file__), "..", self.GENERATED_CODE_DIR)
+        full_path = os.path.join(base_dir, relative_path)
+        
+        os.makedirs(os.path.dirname(full_path), exist_ok=True)
+        
+        # Guardar archivo
+        with open(full_path, "w") as f:
+            f.write(full_code)
+        
+        self._generated_files.append(full_path)
+        logger.info(f"[ACT] Saved generated code: {full_path}")
+        
+        return full_path
+    
+    def _to_class_name(self, target: str) -> str:
+        """Convierte target a nombre de clase.
+        
+        Args:
+            target: Nombre del target (ej: "bigquery", "cloud-run")
+            
+        Returns:
+            Nombre de clase (ej: "BigQuery", "CloudRun")
+        """
+        # Remover prefijos comunes
+        name = target.lower()
+        for prefix in ["google-cloud-", "google-", "cloud-", "gcp-"]:
+            if name.startswith(prefix):
+                name = name[len(prefix):]
+        
+        # Convertir a CamelCase
+        parts = name.replace("-", "_").replace(".", "_").split("_")
+        return "".join(part.capitalize() for part in parts)
+    
+    def get_generated_files(self) -> List[str]:
+        """Retorna lista de archivos generados.
+        
+        Returns:
+            Lista de rutas de archivos generados
+        """
+        return self._generated_files.copy()
+{%- endif %}
diff --git a/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/genesis/core.py b/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/genesis/core.py
new file mode 100644
index 0000000..7fe1a3d
--- /dev/null
+++ b/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/genesis/core.py
@@ -0,0 +1,387 @@
+{%- if cookiecutter.use_google_adk == 'y' and cookiecutter.use_google_cloud == 'y' %}
+"""GENESIS Core - Sistema Autopoietico Central.
+
+Este modulo implementa el nucleo de GENESIS, un sistema que:
+1. Se auto-descubre en su entorno GCP
+2. Razona sobre que acciones tomar
+3. Genera y ejecuta codigo automaticamente
+4. Persiste su estado para continuidad
+5. Mejora su propio codigo periodicamente
+
+El ciclo de vida sigue el patron OODA (Observe, Orient, Decide, Act)
+adaptado para sistemas autopoieticos.
+"""
+import asyncio
+import logging
+import time
+from dataclasses import dataclass, field
+from typing import Optional, List, Any
+from datetime import datetime
+import hashlib
+
+from .perceive import PerceiveModule, EnvironmentContext
+from .think import ThinkModule, ActionPlan
+from .act import ActModule, ActionResult
+from .memory import MemoryModule
+from .evolve import EvolveModule
+
+logger = logging.getLogger(__name__)
+
+
+@dataclass
+class CycleResult:
+    """Resultado de un ciclo GENESIS.
+    
+    Attributes:
+        cycle_id: Identificador unico del ciclo
+        timestamp: Momento de ejecucion
+        context_hash: Hash del contexto para detectar cambios
+        plan_summary: Resumen del plan ejecutado
+        actions_taken: Lista de acciones ejecutadas
+        success: Si el ciclo fue exitoso
+        duration_ms: Duracion en milisegundos
+        evolved: Si se ejecuto evolucion
+        errors: Lista de errores encontrados
+    """
+    cycle_id: str
+    timestamp: datetime
+    context_hash: str
+    plan_summary: str
+    actions_taken: List[str]
+    success: bool
+    duration_ms: float
+    evolved: bool = False
+    errors: List[str] = field(default_factory=list)
+    
+    def to_dict(self) -> dict:
+        """Convierte a diccionario para persistencia."""
+        return {
+            "cycle_id": self.cycle_id,
+            "timestamp": self.timestamp.isoformat(),
+            "context_hash": self.context_hash,
+            "plan_summary": self.plan_summary,
+            "actions_taken": self.actions_taken,
+            "success": self.success,
+            "duration_ms": self.duration_ms,
+            "evolved": self.evolved,
+            "errors": self.errors,
+        }
+
+
+class GenesisCore:
+    """Sistema autopoietico central que vive en Google Cloud.
+    
+    GenesisCore es el cerebro del sistema GENESIS. Coordina todos los
+    modulos (Perceive, Think, Act, Memory, Evolve) en un ciclo continuo
+    de auto-mejora y adaptacion.
+    
+    Ciclo de vida:
+        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
+        â”‚  PERCEIVE â†’ THINK â†’ ACT â†’ REMEMBER      â”‚
+        â”‚       â†‘                      â”‚          â”‚
+        â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€ EVOLVE â”€â”€â”€â”€â”€â”€â”˜          â”‚
+        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
+    
+    El sistema:
+    - Auto-descubre su entorno GCP usando el plugin system
+    - Razona sobre que hacer usando Gemini
+    - Genera y ejecuta codigo para nuevos agentes/plugins
+    - Persiste estado en Firestore para continuidad
+    - Mejora su propio codigo periodicamente
+    
+    Example:
+        >>> genesis = GenesisCore()
+        >>> 
+        >>> # Ejecutar un ciclo
+        >>> result = await genesis.run_cycle()
+        >>> print(f"Success: {result.success}")
+        >>> 
+        >>> # Ejecutar con tarea especifica
+        >>> result = await genesis.run_cycle(task="Crear agente para BigQuery")
+        >>> 
+        >>> # Ejecutar continuamente
+        >>> await genesis.run_continuous(interval_seconds=60)
+    
+    Attributes:
+        perceive: Modulo de percepcion del entorno
+        think: Modulo de razonamiento con Gemini
+        act: Modulo de ejecucion de acciones
+        memory: Modulo de persistencia
+        evolve: Modulo de auto-mejora
+    """
+    
+    # Configuracion por defecto
+    DEFAULT_EVOLUTION_THRESHOLD = 10  # Evolucionar cada N ciclos
+    DEFAULT_CYCLE_INTERVAL = 60  # Segundos entre ciclos continuos
+    
+    def __init__(
+        self,
+        evolution_threshold: int = DEFAULT_EVOLUTION_THRESHOLD,
+        auto_evolve: bool = True,
+    ):
+        """Inicializa GenesisCore.
+        
+        Args:
+            evolution_threshold: Numero de ciclos entre evoluciones
+            auto_evolve: Si debe evolucionar automaticamente
+        """
+        logger.info("Initializing GENESIS Core...")
+        
+        # Inicializar modulos
+        self.perceive = PerceiveModule()
+        self.think = ThinkModule()
+        self.act = ActModule()
+        self.memory = MemoryModule()
+        self.evolve = EvolveModule()
+        
+        # Inyectar dependencias entre modulos
+        self.act.think = self.think
+        self.evolve.think = self.think
+        self.evolve.act = self.act
+        self.evolve.memory = self.memory
+        
+        # Configuracion
+        self._evolution_threshold = evolution_threshold
+        self._auto_evolve = auto_evolve
+        self._cycle_count = 0
+        self._start_time = datetime.utcnow()
+        
+        logger.info("GENESIS Core initialized successfully")
+    
+    async def run_cycle(self, task: Optional[str] = None) -> CycleResult:
+        """Ejecuta un ciclo completo GENESIS.
+        
+        Un ciclo consiste en:
+        1. PERCEIVE - Escanear y entender el entorno GCP
+        2. THINK - Razonar sobre que acciones tomar
+        3. ACT - Ejecutar las acciones planificadas
+        4. REMEMBER - Persistir el resultado
+        5. EVOLVE - Mejorar el sistema (periodicamente)
+        
+        Args:
+            task: Tarea opcional especifica. Si es None, el sistema
+                  auto-determina que hacer basado en el contexto.
+        
+        Returns:
+            CycleResult con el resultado del ciclo
+            
+        Example:
+            >>> result = await genesis.run_cycle()
+            >>> if result.success:
+            ...     print(f"Ejecutadas {len(result.actions_taken)} acciones")
+            >>> else:
+            ...     print(f"Errores: {result.errors}")
+        """
+        cycle_id = self._generate_cycle_id()
+        start_time = time.time()
+        errors: List[str] = []
+        
+        logger.info(f"[GENESIS] Starting cycle {cycle_id}")
+        
+        try:
+            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
+            # FASE 1: PERCEIVE - Escanear entorno
+            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
+            logger.info("[GENESIS] Phase 1: PERCEIVE")
+            try:
+                context = await self.perceive.scan()
+                if task:
+                    context.user_task = task
+                logger.info(f"[GENESIS] Context hash: {context.hash()}")
+            except Exception as e:
+                logger.error(f"[GENESIS] Perceive failed: {e}")
+                errors.append(f"perceive: {str(e)}")
+                # Crear contexto minimo para continuar
+                context = EnvironmentContext.empty()
+                context.user_task = task
+            
+            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
+            # FASE 2: THINK - Razonar sobre que hacer
+            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
+            logger.info("[GENESIS] Phase 2: THINK")
+            try:
+                plan = await self.think.reason(context)
+                logger.info(f"[GENESIS] Plan: {len(plan.actions)} actions")
+            except Exception as e:
+                logger.error(f"[GENESIS] Think failed: {e}")
+                errors.append(f"think: {str(e)}")
+                plan = ActionPlan.empty()
+            
+            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
+            # FASE 3: ACT - Ejecutar acciones
+            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
+            logger.info("[GENESIS] Phase 3: ACT")
+            try:
+                result = await self.act.execute(plan)
+                logger.info(f"[GENESIS] Actions executed: {result.success}")
+                if result.errors:
+                    errors.extend(result.errors)
+            except Exception as e:
+                logger.error(f"[GENESIS] Act failed: {e}")
+                errors.append(f"act: {str(e)}")
+                result = ActionResult.empty()
+            
+            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
+            # FASE 4: REMEMBER - Persistir estado
+            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
+            logger.info("[GENESIS] Phase 4: REMEMBER")
+            try:
+                await self.memory.store_cycle(
+                    cycle_id=cycle_id,
+                    context=context,
+                    plan=plan,
+                    result=result,
+                )
+            except Exception as e:
+                logger.warning(f"[GENESIS] Memory store failed: {e}")
+                errors.append(f"memory: {str(e)}")
+            
+            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
+            # FASE 5: EVOLVE - Auto-mejora (periodica)
+            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
+            self._cycle_count += 1
+            evolved = False
+            
+            if self._auto_evolve and self._should_evolve():
+                logger.info("[GENESIS] Phase 5: EVOLVE")
+                try:
+                    await self.evolve.improve()
+                    evolved = True
+                except Exception as e:
+                    logger.warning(f"[GENESIS] Evolve failed: {e}")
+                    errors.append(f"evolve: {str(e)}")
+            
+            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
+            # Construir resultado
+            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
+            duration_ms = (time.time() - start_time) * 1000
+            
+            cycle_result = CycleResult(
+                cycle_id=cycle_id,
+                timestamp=datetime.utcnow(),
+                context_hash=context.hash(),
+                plan_summary=plan.reasoning[:200] if plan.reasoning else "",
+                actions_taken=result.actions,
+                success=len(errors) == 0 and result.success,
+                duration_ms=duration_ms,
+                evolved=evolved,
+                errors=errors,
+            )
+            
+            logger.info(
+                f"[GENESIS] Cycle {cycle_id} completed: "
+                f"success={cycle_result.success}, "
+                f"actions={len(result.actions)}, "
+                f"duration={duration_ms:.2f}ms"
+            )
+            
+            return cycle_result
+            
+        except Exception as e:
+            # Error catastrofico - registrar y crear resultado de error
+            logger.critical(f"[GENESIS] Critical error in cycle: {e}")
+            duration_ms = (time.time() - start_time) * 1000
+            
+            return CycleResult(
+                cycle_id=cycle_id,
+                timestamp=datetime.utcnow(),
+                context_hash="error",
+                plan_summary="",
+                actions_taken=[],
+                success=False,
+                duration_ms=duration_ms,
+                evolved=False,
+                errors=[f"critical: {str(e)}"],
+            )
+    
+    async def run_continuous(
+        self,
+        interval_seconds: float = DEFAULT_CYCLE_INTERVAL,
+        max_cycles: Optional[int] = None,
+    ) -> None:
+        """Ejecuta ciclos GENESIS continuamente.
+        
+        El sistema ejecutara ciclos indefinidamente (o hasta max_cycles)
+        con el intervalo especificado entre ciclos.
+        
+        Args:
+            interval_seconds: Segundos entre ciclos
+            max_cycles: Numero maximo de ciclos (None = infinito)
+            
+        Example:
+            >>> # Ejecutar indefinidamente
+            >>> await genesis.run_continuous()
+            >>> 
+            >>> # Ejecutar 100 ciclos
+            >>> await genesis.run_continuous(max_cycles=100)
+        """
+        logger.info(
+            f"[GENESIS] Starting continuous mode: "
+            f"interval={interval_seconds}s, max_cycles={max_cycles}"
+        )
+        
+        cycles_run = 0
+        
+        while max_cycles is None or cycles_run < max_cycles:
+            try:
+                result = await self.run_cycle()
+                cycles_run += 1
+                
+                status = "âœ“" if result.success else "âœ—"
+                logger.info(
+                    f"[GENESIS] Continuous cycle {cycles_run}: "
+                    f"{status} ({result.duration_ms:.0f}ms)"
+                )
+                
+            except Exception as e:
+                logger.error(f"[GENESIS] Continuous cycle error: {e}")
+            
+            # Esperar antes del siguiente ciclo
+            await asyncio.sleep(interval_seconds)
+        
+        logger.info(f"[GENESIS] Continuous mode ended after {cycles_run} cycles")
+    
+    async def force_evolve(self) -> bool:
+        """Fuerza un ciclo de evolucion inmediato.
+        
+        Returns:
+            True si la evolucion fue exitosa
+        """
+        logger.info("[GENESIS] Forcing evolution cycle")
+        try:
+            await self.evolve.improve()
+            return True
+        except Exception as e:
+            logger.error(f"[GENESIS] Forced evolution failed: {e}")
+            return False
+    
+    def _should_evolve(self) -> bool:
+        """Determina si debe ejecutarse evolucion."""
+        return self._cycle_count % self._evolution_threshold == 0
+    
+    def _generate_cycle_id(self) -> str:
+        """Genera ID unico para el ciclo."""
+        timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
+        unique = hashlib.sha256(
+            f"{timestamp}_{self._cycle_count}".encode()
+        ).hexdigest()[:8]
+        return f"cycle_{timestamp}_{unique}"
+    
+    def get_status(self) -> dict:
+        """Obtiene estado actual del sistema.
+        
+        Returns:
+            Diccionario con metricas del sistema
+        """
+        uptime = (datetime.utcnow() - self._start_time).total_seconds()
+        
+        return {
+            "status": "running",
+            "uptime_seconds": uptime,
+            "cycles_completed": self._cycle_count,
+            "next_evolution_in": self._evolution_threshold - (
+                self._cycle_count % self._evolution_threshold
+            ),
+            "auto_evolve": self._auto_evolve,
+        }
+{%- endif %}
diff --git a/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/genesis/evolve.py b/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/genesis/evolve.py
new file mode 100644
index 0000000..34bd621
--- /dev/null
+++ b/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/genesis/evolve.py
@@ -0,0 +1,417 @@
+{%- if cookiecutter.use_google_adk == 'y' and cookiecutter.use_google_cloud == 'y' %}
+"""Modulo de Evolucion - Auto-mejora del sistema.
+
+Este modulo implementa la fase EVOLVE del ciclo GENESIS.
+Analiza el rendimiento del sistema y genera mejoras automaticas.
+"""
+import ast
+import logging
+from dataclasses import dataclass, field
+from datetime import datetime
+from typing import List, Dict, Any, Optional
+
+logger = logging.getLogger(__name__)
+
+
+@dataclass
+class Improvement:
+    """Mejora propuesta para el sistema.
+    
+    Attributes:
+        file: Archivo a modificar
+        description: Descripcion de la mejora
+        code: Codigo nuevo/modificado
+        confidence: Confianza en la mejora (0-1)
+        impact: Impacto estimado (low/medium/high)
+    """
+    file: str
+    description: str
+    code: str
+    confidence: float = 0.5
+    impact: str = "medium"
+    
+    def to_dict(self) -> dict:
+        """Convierte a diccionario."""
+        return {
+            "file": self.file,
+            "description": self.description,
+            "code_preview": self.code[:200] + "..." if len(self.code) > 200 else self.code,
+            "confidence": self.confidence,
+            "impact": self.impact,
+        }
+
+
+@dataclass
+class EvolutionResult:
+    """Resultado de un ciclo de evolucion.
+    
+    Attributes:
+        improvements_proposed: Mejoras propuestas
+        improvements_applied: Mejoras aplicadas
+        success: Si la evolucion fue exitosa
+        timestamp: Momento de la evolucion
+    """
+    improvements_proposed: List[Improvement] = field(default_factory=list)
+    improvements_applied: List[Improvement] = field(default_factory=list)
+    success: bool = True
+    timestamp: datetime = field(default_factory=datetime.utcnow)
+    
+    def to_dict(self) -> dict:
+        """Convierte a diccionario."""
+        return {
+            "improvements_proposed": len(self.improvements_proposed),
+            "improvements_applied": len(self.improvements_applied),
+            "success": self.success,
+            "timestamp": self.timestamp.isoformat(),
+            "applied": [i.to_dict() for i in self.improvements_applied],
+        }
+
+
+class EvolveModule:
+    """Modulo de auto-mejora del sistema.
+    
+    Analiza metricas y errores para proponer y aplicar
+    mejoras automaticas al codigo del sistema.
+    
+    Example:
+        >>> evolve = EvolveModule()
+        >>> evolve.think = think_module
+        >>> evolve.memory = memory_module
+        >>> result = await evolve.improve()
+        >>> print(f"Applied {len(result.improvements_applied)} improvements")
+    """
+    
+    # Prompt para analisis de evolucion
+    EVOLUTION_PROMPT = '''Eres GENESIS en modo EVOLUCION.
+
+Analiza las metricas y errores del sistema para proponer mejoras.
+
+## Metricas Actuales
+{metrics}
+
+## Errores Recientes
+{errors}
+
+## Codigo Actual (fragmento relevante)
+{code_context}
+
+REGLAS:
+1. Solo proponer mejoras que mejoren metricas concretas
+2. Las mejoras deben ser pequeÃ±as e incrementales
+3. Priorizar correccion de errores recurrentes
+4. No modificar logica core sin razon clara
+5. Mantener compatibilidad hacia atras
+
+Responde en JSON:
+{{
+    "analysis": "Analisis de la situacion actual",
+    "improvements": [
+        {{
+            "file": "ruta/al/archivo.py",
+            "description": "Que mejora y por que",
+            "code": "codigo python completo del archivo mejorado",
+            "confidence": 0.0-1.0,
+            "impact": "low|medium|high"
+        }}
+    ]
+}}
+
+Si no hay mejoras necesarias, retorna improvements como lista vacia.
+'''
+
+    # Umbral de confianza para aplicar mejoras automaticamente
+    AUTO_APPLY_CONFIDENCE = 0.8
+    
+    # Maximo de mejoras a aplicar por ciclo
+    MAX_IMPROVEMENTS_PER_CYCLE = 3
+    
+    def __init__(self):
+        """Inicializa el modulo de evolucion."""
+        self.think = None  # Inyectado por GenesisCore
+        self.act = None    # Inyectado por GenesisCore
+        self.memory = None # Inyectado por GenesisCore
+        self._evolution_count = 0
+        logger.info("EvolveModule initialized")
+    
+    async def improve(self) -> EvolutionResult:
+        """Ejecuta ciclo de auto-mejora.
+        
+        Analiza el estado del sistema y aplica mejoras
+        automaticas cuando la confianza es alta.
+        
+        Returns:
+            EvolutionResult con mejoras aplicadas
+        """
+        logger.info("[EVOLVE] Starting evolution cycle...")
+        self._evolution_count += 1
+        
+        try:
+            # Obtener metricas y errores
+            metrics = await self._get_metrics()
+            errors = await self._get_recent_errors()
+            
+            # Obtener contexto de codigo relevante
+            code_context = await self._get_code_context(errors)
+            
+            # Proponer mejoras
+            improvements = await self._propose_improvements(
+                metrics, errors, code_context
+            )
+            
+            logger.info(f"[EVOLVE] Proposed {len(improvements)} improvements")
+            
+            # Filtrar y aplicar mejoras con alta confianza
+            applied = []
+            for imp in improvements[:self.MAX_IMPROVEMENTS_PER_CYCLE]:
+                if imp.confidence >= self.AUTO_APPLY_CONFIDENCE:
+                    if await self._validate_improvement(imp):
+                        if await self._apply_improvement(imp):
+                            applied.append(imp)
+                            logger.info(
+                                f"[EVOLVE] Applied: {imp.description[:50]}..."
+                            )
+            
+            logger.info(f"[EVOLVE] Applied {len(applied)} improvements")
+            
+            return EvolutionResult(
+                improvements_proposed=improvements,
+                improvements_applied=applied,
+                success=True,
+            )
+            
+        except Exception as e:
+            logger.error(f"[EVOLVE] Evolution failed: {e}")
+            return EvolutionResult(
+                success=False,
+            )
+    
+    async def _get_metrics(self) -> Dict[str, Any]:
+        """Obtiene metricas del sistema.
+        
+        Returns:
+            Diccionario con metricas
+        """
+        if self.memory is None:
+            return {"status": "memory_unavailable"}
+        
+        try:
+            return await self.memory.get_metrics()
+        except Exception as e:
+            logger.warning(f"Could not get metrics: {e}")
+            return {"error": str(e)}
+    
+    async def _get_recent_errors(self) -> List[str]:
+        """Obtiene errores recientes.
+        
+        Returns:
+            Lista de errores
+        """
+        if self.memory is None:
+            return []
+        
+        try:
+            state = await self.memory.get_state()
+            return state.errors_recent
+        except Exception as e:
+            logger.warning(f"Could not get errors: {e}")
+            return []
+    
+    async def _get_code_context(self, errors: List[str]) -> str:
+        """Obtiene contexto de codigo relevante.
+        
+        Analiza errores para identificar archivos relevantes
+        y extrae fragmentos de codigo.
+        
+        Args:
+            errors: Lista de errores recientes
+            
+        Returns:
+            Fragmentos de codigo como string
+        """
+        # Identificar archivos mencionados en errores
+        import re
+        
+        files_mentioned = set()
+        for error in errors:
+            # Buscar patrones de archivos Python
+            matches = re.findall(r'[\w/]+\.py', error)
+            files_mentioned.update(matches)
+        
+        if not files_mentioned:
+            return "No specific files identified from errors."
+        
+        # Por seguridad, solo incluir archivos del sistema GENESIS
+        context_parts = []
+        for file in list(files_mentioned)[:3]:  # Max 3 archivos
+            if "genesis" in file.lower():
+                context_parts.append(f"# File: {file}")
+                context_parts.append("# [Code content would be here]")
+        
+        return "\n\n".join(context_parts) if context_parts else "No GENESIS files in errors."
+    
+    async def _propose_improvements(
+        self,
+        metrics: Dict[str, Any],
+        errors: List[str],
+        code_context: str,
+    ) -> List[Improvement]:
+        """Propone mejoras basadas en analisis.
+        
+        Args:
+            metrics: Metricas del sistema
+            errors: Errores recientes
+            code_context: Contexto de codigo
+            
+        Returns:
+            Lista de mejoras propuestas
+        """
+        if self.think is None:
+            logger.warning("[EVOLVE] ThinkModule not available")
+            return []
+        
+        import json
+        
+        prompt = self.EVOLUTION_PROMPT.format(
+            metrics=json.dumps(metrics, indent=2),
+            errors=json.dumps(errors[:10], indent=2),  # Limitar errores
+            code_context=code_context[:2000],  # Limitar contexto
+        )
+        
+        try:
+            response = await self.think.agent.run(prompt)
+            
+            # Parsear respuesta
+            data = self._parse_response(response)
+            
+            improvements = []
+            for imp_data in data.get("improvements", []):
+                improvements.append(Improvement(
+                    file=imp_data.get("file", ""),
+                    description=imp_data.get("description", ""),
+                    code=imp_data.get("code", ""),
+                    confidence=imp_data.get("confidence", 0.5),
+                    impact=imp_data.get("impact", "medium"),
+                ))
+            
+            return improvements
+            
+        except Exception as e:
+            logger.error(f"[EVOLVE] Failed to propose improvements: {e}")
+            return []
+    
+    def _parse_response(self, response: str) -> Dict[str, Any]:
+        """Parsea respuesta JSON de Gemini.
+        
+        Args:
+            response: Respuesta raw
+            
+        Returns:
+            Diccionario parseado
+        """
+        import json
+        import re
+        
+        # Intentar parsear directamente
+        try:
+            return json.loads(response)
+        except json.JSONDecodeError:
+            pass
+        
+        # Buscar bloque JSON
+        json_match = re.search(r'\{.*\}', response, re.DOTALL)
+        if json_match:
+            try:
+                return json.loads(json_match.group())
+            except json.JSONDecodeError:
+                pass
+        
+        return {"improvements": []}
+    
+    async def _validate_improvement(self, improvement: Improvement) -> bool:
+        """Valida que una mejora es segura de aplicar.
+        
+        Args:
+            improvement: Mejora a validar
+            
+        Returns:
+            True si es segura
+        """
+        # Verificar que el codigo es valido
+        try:
+            ast.parse(improvement.code)
+        except SyntaxError as e:
+            logger.warning(f"[EVOLVE] Invalid syntax in improvement: {e}")
+            return False
+        
+        # Verificar que el archivo objetivo es del sistema GENESIS
+        if not any(
+            pattern in improvement.file.lower()
+            for pattern in ["genesis", "agents", "core", "cloud"]
+        ):
+            logger.warning(f"[EVOLVE] Target file not in allowed paths: {improvement.file}")
+            return False
+        
+        # Verificar que no es un archivo critico
+        critical_files = ["__init__.py", "core.py", "config.py"]
+        if any(cf in improvement.file for cf in critical_files):
+            # Solo permitir con alta confianza
+            if improvement.confidence < 0.9:
+                logger.warning(f"[EVOLVE] Critical file requires higher confidence")
+                return False
+        
+        return True
+    
+    async def _apply_improvement(self, improvement: Improvement) -> bool:
+        """Aplica una mejora al sistema.
+        
+        Args:
+            improvement: Mejora a aplicar
+            
+        Returns:
+            True si se aplico exitosamente
+        """
+        import os
+        
+        try:
+            # Crear backup
+            if os.path.exists(improvement.file):
+                backup_path = f"{improvement.file}.backup.{self._evolution_count}"
+                with open(improvement.file, "r") as f:
+                    original = f.read()
+                with open(backup_path, "w") as f:
+                    f.write(original)
+                logger.debug(f"[EVOLVE] Created backup: {backup_path}")
+            
+            # Escribir nuevo codigo
+            os.makedirs(os.path.dirname(improvement.file), exist_ok=True)
+            with open(improvement.file, "w") as f:
+                f.write(improvement.code)
+            
+            logger.info(f"[EVOLVE] Applied improvement to: {improvement.file}")
+            
+            # Registrar en memoria
+            if self.memory:
+                await self.memory.store_agent(
+                    f"evolution_{self._evolution_count}",
+                    {
+                        "type": "evolution",
+                        "file": improvement.file,
+                        "description": improvement.description,
+                        "confidence": improvement.confidence,
+                    },
+                )
+            
+            return True
+            
+        except Exception as e:
+            logger.error(f"[EVOLVE] Failed to apply improvement: {e}")
+            return False
+    
+    def get_evolution_count(self) -> int:
+        """Retorna numero de evoluciones ejecutadas.
+        
+        Returns:
+            Contador de evoluciones
+        """
+        return self._evolution_count
+{%- endif %}
diff --git a/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/genesis/memory.py b/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/genesis/memory.py
new file mode 100644
index 0000000..7c5047b
--- /dev/null
+++ b/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/genesis/memory.py
@@ -0,0 +1,391 @@
+{%- if cookiecutter.use_google_adk == 'y' and cookiecutter.use_google_cloud == 'y' %}
+"""Modulo de Memoria - Persistencia en Firestore.
+
+Este modulo implementa la persistencia del estado de GENESIS
+usando Google Cloud Firestore como backend.
+"""
+import logging
+from dataclasses import dataclass, field
+from datetime import datetime
+from typing import Dict, Any, List, Optional
+
+logger = logging.getLogger(__name__)
+
+
+@dataclass
+class MemoryState:
+    """Estado de memoria del sistema.
+    
+    Attributes:
+        total_cycles: Total de ciclos ejecutados
+        success_rate: Tasa de exito (0-1)
+        agents_generated: Numero de agentes generados
+        plugins_generated: Numero de plugins generados
+        last_cycle: Info del ultimo ciclo
+        last_evolution: Info de la ultima evolucion
+        errors_recent: Errores recientes
+    """
+    total_cycles: int = 0
+    success_rate: float = 0.0
+    agents_generated: int = 0
+    plugins_generated: int = 0
+    last_cycle: Optional[Dict[str, Any]] = None
+    last_evolution: Optional[Dict[str, Any]] = None
+    errors_recent: List[str] = field(default_factory=list)
+    
+    def to_dict(self) -> dict:
+        """Convierte a diccionario."""
+        return {
+            "total_cycles": self.total_cycles,
+            "success_rate": self.success_rate,
+            "agents_generated": self.agents_generated,
+            "plugins_generated": self.plugins_generated,
+            "last_cycle": self.last_cycle,
+            "last_evolution": self.last_evolution,
+            "errors_recent": self.errors_recent,
+        }
+
+
+class MemoryModule:
+    """Modulo de memoria persistente.
+    
+    Utiliza Firestore para persistir:
+    - Historial de ciclos
+    - Agentes generados
+    - Plugins generados
+    - Metricas del sistema
+    
+    Example:
+        >>> memory = MemoryModule()
+        >>> await memory.store_cycle(cycle_id, context, plan, result)
+        >>> state = await memory.get_state()
+        >>> print(f"Total cycles: {state.total_cycles}")
+    """
+    
+    # Colecciones de Firestore
+    COLLECTION_CYCLES = "genesis_cycles"
+    COLLECTION_AGENTS = "genesis_agents"
+    COLLECTION_PLUGINS = "genesis_plugins"
+    COLLECTION_STATE = "genesis_state"
+    
+    # Limite de ciclos a mantener en memoria
+    MAX_CYCLES_HISTORY = 1000
+    MAX_RECENT_ERRORS = 50
+    
+    def __init__(self):
+        """Inicializa el modulo de memoria."""
+        self._client = None
+        self._local_cache: Dict[str, Any] = {}
+        self._use_local = False
+        logger.info("MemoryModule initialized")
+    
+    @property
+    def client(self):
+        """Lazy initialization del cliente Firestore."""
+        if self._client is None:
+            try:
+                from ..cloud.firestore import FirestoreClient
+                self._client = FirestoreClient()
+                logger.info("Firestore client initialized")
+            except ImportError:
+                logger.warning("Firestore not available, using local cache")
+                self._use_local = True
+            except Exception as e:
+                logger.warning(f"Firestore init failed: {e}, using local cache")
+                self._use_local = True
+        return self._client
+    
+    async def store_cycle(
+        self,
+        cycle_id: str,
+        context,
+        plan,
+        result,
+    ) -> None:
+        """Almacena resultado de un ciclo.
+        
+        Args:
+            cycle_id: ID del ciclo
+            context: EnvironmentContext del ciclo
+            plan: ActionPlan ejecutado
+            result: ActionResult del ciclo
+        """
+        doc = {
+            "cycle_id": cycle_id,
+            "timestamp": datetime.utcnow().isoformat(),
+            "context_hash": context.hash(),
+            "context_summary": {
+                "project_id": context.project_id,
+                "services_count": len(context.services),
+                "resources_count": len(context.resources),
+                "changes_count": len(context.changes),
+                "user_task": context.user_task,
+            },
+            "plan_summary": {
+                "reasoning": plan.reasoning[:500] if plan.reasoning else "",
+                "actions_count": len(plan.actions),
+                "confidence": plan.confidence,
+            },
+            "result": result.to_dict(),
+        }
+        
+        if self._use_local:
+            self._store_local("cycles", cycle_id, doc)
+        else:
+            try:
+                await self.client.add(self.COLLECTION_CYCLES, doc)
+            except Exception as e:
+                logger.error(f"Failed to store cycle: {e}")
+                self._store_local("cycles", cycle_id, doc)
+        
+        logger.debug(f"Stored cycle: {cycle_id}")
+    
+    async def store_agent(
+        self,
+        agent_name: str,
+        agent_info: Dict[str, Any],
+    ) -> None:
+        """Almacena informacion de un agente generado.
+        
+        Args:
+            agent_name: Nombre del agente
+            agent_info: Informacion del agente
+        """
+        doc = {
+            "name": agent_name,
+            "created_at": datetime.utcnow().isoformat(),
+            **agent_info,
+        }
+        
+        if self._use_local:
+            self._store_local("agents", agent_name, doc)
+        else:
+            try:
+                await self.client.set(
+                    f"{self.COLLECTION_AGENTS}/{agent_name}",
+                    doc,
+                )
+            except Exception as e:
+                logger.error(f"Failed to store agent: {e}")
+                self._store_local("agents", agent_name, doc)
+    
+    async def store_plugin(
+        self,
+        plugin_name: str,
+        plugin_info: Dict[str, Any],
+    ) -> None:
+        """Almacena informacion de un plugin generado.
+        
+        Args:
+            plugin_name: Nombre del plugin
+            plugin_info: Informacion del plugin
+        """
+        doc = {
+            "name": plugin_name,
+            "created_at": datetime.utcnow().isoformat(),
+            **plugin_info,
+        }
+        
+        if self._use_local:
+            self._store_local("plugins", plugin_name, doc)
+        else:
+            try:
+                await self.client.set(
+                    f"{self.COLLECTION_PLUGINS}/{plugin_name}",
+                    doc,
+                )
+            except Exception as e:
+                logger.error(f"Failed to store plugin: {e}")
+                self._store_local("plugins", plugin_name, doc)
+    
+    async def get_state(self) -> MemoryState:
+        """Obtiene estado actual del sistema.
+        
+        Returns:
+            MemoryState con metricas del sistema
+        """
+        if self._use_local:
+            return self._get_local_state()
+        
+        try:
+            # Obtener ciclos recientes
+            cycles = await self.client.query(
+                self.COLLECTION_CYCLES,
+                order_by="timestamp",
+                order_direction="desc",
+                limit=self.MAX_CYCLES_HISTORY,
+            )
+            
+            # Obtener agentes
+            agents = await self.client.list(self.COLLECTION_AGENTS)
+            
+            # Obtener plugins
+            plugins = await self.client.list(self.COLLECTION_PLUGINS)
+            
+            # Calcular metricas
+            total_cycles = len(cycles)
+            successes = sum(
+                1 for c in cycles
+                if c.get("result", {}).get("success", False)
+            )
+            success_rate = successes / total_cycles if total_cycles > 0 else 0.0
+            
+            # Obtener errores recientes
+            errors = []
+            for c in cycles[:20]:  # Ultimos 20 ciclos
+                cycle_errors = c.get("result", {}).get("errors", [])
+                errors.extend(cycle_errors[:5])  # Max 5 errores por ciclo
+            
+            return MemoryState(
+                total_cycles=total_cycles,
+                success_rate=success_rate,
+                agents_generated=len(agents),
+                plugins_generated=len(plugins),
+                last_cycle=cycles[0] if cycles else None,
+                errors_recent=errors[:self.MAX_RECENT_ERRORS],
+            )
+            
+        except Exception as e:
+            logger.error(f"Failed to get state: {e}")
+            return self._get_local_state()
+    
+    async def get_cycles(
+        self,
+        limit: int = 100,
+        only_successful: bool = False,
+    ) -> List[Dict[str, Any]]:
+        """Obtiene historial de ciclos.
+        
+        Args:
+            limit: Numero maximo de ciclos
+            only_successful: Si solo retornar ciclos exitosos
+            
+        Returns:
+            Lista de ciclos
+        """
+        if self._use_local:
+            cycles = list(self._local_cache.get("cycles", {}).values())
+            if only_successful:
+                cycles = [
+                    c for c in cycles
+                    if c.get("result", {}).get("success", False)
+                ]
+            return sorted(
+                cycles,
+                key=lambda x: x.get("timestamp", ""),
+                reverse=True,
+            )[:limit]
+        
+        try:
+            cycles = await self.client.query(
+                self.COLLECTION_CYCLES,
+                order_by="timestamp",
+                order_direction="desc",
+                limit=limit,
+            )
+            
+            if only_successful:
+                cycles = [
+                    c for c in cycles
+                    if c.get("result", {}).get("success", False)
+                ]
+            
+            return cycles
+            
+        except Exception as e:
+            logger.error(f"Failed to get cycles: {e}")
+            return []
+    
+    async def get_metrics(self) -> Dict[str, Any]:
+        """Obtiene metricas detalladas del sistema.
+        
+        Returns:
+            Diccionario con metricas
+        """
+        state = await self.get_state()
+        cycles = await self.get_cycles(limit=100)
+        
+        # Calcular metricas adicionales
+        if cycles:
+            durations = [
+                c.get("result", {}).get("duration_ms", 0)
+                for c in cycles
+            ]
+            avg_duration = sum(durations) / len(durations) if durations else 0
+            
+            actions_counts = [
+                len(c.get("result", {}).get("actions", []))
+                for c in cycles
+            ]
+            avg_actions = sum(actions_counts) / len(actions_counts) if actions_counts else 0
+        else:
+            avg_duration = 0
+            avg_actions = 0
+        
+        return {
+            "total_cycles": state.total_cycles,
+            "success_rate": state.success_rate,
+            "agents_generated": state.agents_generated,
+            "plugins_generated": state.plugins_generated,
+            "avg_cycle_duration_ms": avg_duration,
+            "avg_actions_per_cycle": avg_actions,
+            "recent_errors_count": len(state.errors_recent),
+        }
+    
+    def _store_local(self, collection: str, doc_id: str, doc: dict) -> None:
+        """Almacena documento en cache local.
+        
+        Args:
+            collection: Nombre de la coleccion
+            doc_id: ID del documento
+            doc: Documento a almacenar
+        """
+        if collection not in self._local_cache:
+            self._local_cache[collection] = {}
+        
+        self._local_cache[collection][doc_id] = doc
+        
+        # Limpiar cache si es muy grande
+        if len(self._local_cache[collection]) > self.MAX_CYCLES_HISTORY:
+            # Mantener solo los mas recientes
+            items = sorted(
+                self._local_cache[collection].items(),
+                key=lambda x: x[1].get("timestamp", ""),
+                reverse=True,
+            )
+            self._local_cache[collection] = dict(items[:self.MAX_CYCLES_HISTORY])
+    
+    def _get_local_state(self) -> MemoryState:
+        """Obtiene estado desde cache local.
+        
+        Returns:
+            MemoryState desde cache local
+        """
+        cycles = list(self._local_cache.get("cycles", {}).values())
+        agents = list(self._local_cache.get("agents", {}).values())
+        plugins = list(self._local_cache.get("plugins", {}).values())
+        
+        total = len(cycles)
+        successes = sum(
+            1 for c in cycles
+            if c.get("result", {}).get("success", False)
+        )
+        
+        errors = []
+        for c in sorted(
+            cycles,
+            key=lambda x: x.get("timestamp", ""),
+            reverse=True,
+        )[:20]:
+            cycle_errors = c.get("result", {}).get("errors", [])
+            errors.extend(cycle_errors[:5])
+        
+        return MemoryState(
+            total_cycles=total,
+            success_rate=successes / total if total > 0 else 0.0,
+            agents_generated=len(agents),
+            plugins_generated=len(plugins),
+            last_cycle=cycles[-1] if cycles else None,
+            errors_recent=errors[:self.MAX_RECENT_ERRORS],
+        )
+{%- endif %}
diff --git a/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/genesis/perceive.py b/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/genesis/perceive.py
new file mode 100644
index 0000000..464c0dc
--- /dev/null
+++ b/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/genesis/perceive.py
@@ -0,0 +1,334 @@
+{%- if cookiecutter.use_google_adk == 'y' and cookiecutter.use_google_cloud == 'y' %}
+"""Modulo de Percepcion - Detecta y entiende el entorno GCP.
+
+Este modulo implementa la fase PERCEIVE del ciclo GENESIS.
+Se integra con el sistema de plugins de GCP Discovery para
+detectar automaticamente todos los recursos disponibles.
+"""
+import hashlib
+import json
+import logging
+from dataclasses import dataclass, field
+from typing import Dict, List, Any, Optional
+from datetime import datetime
+
+logger = logging.getLogger(__name__)
+
+
+@dataclass
+class EnvironmentContext:
+    """Contexto completo del entorno GCP.
+    
+    Contiene toda la informacion que GENESIS necesita para
+    razonar sobre su entorno y decidir acciones.
+    
+    Attributes:
+        project_id: ID del proyecto GCP
+        region: Region activa
+        services: Servicios habilitados
+        resources: Recursos descubiertos por servicio
+        changes: Cambios detectados desde ultimo scan
+        memory_state: Estado de memoria del sistema
+        user_task: Tarea especifica del usuario (opcional)
+        timestamp: Momento del scan
+    """
+    project_id: str = ""
+    region: str = "us-central1"
+    services: List[str] = field(default_factory=list)
+    resources: Dict[str, Dict[str, Any]] = field(default_factory=dict)
+    changes: List[Dict[str, Any]] = field(default_factory=list)
+    memory_state: Dict[str, Any] = field(default_factory=dict)
+    user_task: Optional[str] = None
+    timestamp: datetime = field(default_factory=datetime.utcnow)
+    
+    def hash(self) -> str:
+        """Genera hash unico del contexto.
+        
+        Util para detectar si el entorno ha cambiado.
+        
+        Returns:
+            Hash SHA256 truncado a 16 caracteres
+        """
+        data = json.dumps({
+            "project_id": self.project_id,
+            "region": self.region,
+            "services": sorted(self.services),
+            "resources": self.resources,
+        }, sort_keys=True)
+        return hashlib.sha256(data.encode()).hexdigest()[:16]
+    
+    def to_prompt(self) -> str:
+        """Convierte contexto a prompt para Gemini.
+        
+        Genera un prompt estructurado que permite a Gemini
+        entender completamente el estado del sistema.
+        
+        Returns:
+            Prompt formateado para razonamiento
+        """
+        services_str = "\n".join(f"  - {s}" for s in self.services[:20])
+        if len(self.services) > 20:
+            services_str += f"\n  ... y {len(self.services) - 20} mas"
+        
+        resources_str = json.dumps(self.resources, indent=2, default=str)
+        changes_str = json.dumps(self.changes, indent=2, default=str)
+        memory_str = json.dumps(self.memory_state, indent=2, default=str)
+        
+        return f'''## Contexto del Entorno GCP
+
+### Proyecto
+- ID: {self.project_id}
+- Region: {self.region}
+- Timestamp: {self.timestamp.isoformat()}
+
+### Servicios Habilitados ({len(self.services)})
+{services_str}
+
+### Recursos Descubiertos
+{resources_str}
+
+### Cambios Detectados
+{changes_str}
+
+### Estado de Memoria
+{memory_str}
+
+### Tarea del Usuario
+{self.user_task or "Auto-determinar siguiente accion optima"}
+
+---
+Analiza este contexto y decide que acciones tomar.
+'''
+    
+    def to_dict(self) -> dict:
+        """Convierte a diccionario para persistencia."""
+        return {
+            "project_id": self.project_id,
+            "region": self.region,
+            "services": self.services,
+            "resources": self.resources,
+            "changes": self.changes,
+            "memory_state": self.memory_state,
+            "user_task": self.user_task,
+            "timestamp": self.timestamp.isoformat(),
+            "hash": self.hash(),
+        }
+    
+    @classmethod
+    def empty(cls) -> "EnvironmentContext":
+        """Crea contexto vacio para casos de error."""
+        return cls(
+            project_id="unknown",
+            changes=[{"type": "error", "message": "Could not scan environment"}],
+        )
+
+
+class PerceiveModule:
+    """Modulo de percepcion del entorno GCP.
+    
+    Se integra con GCPDiscovery para detectar automaticamente
+    todos los recursos disponibles en el proyecto.
+    
+    Example:
+        >>> perceive = PerceiveModule()
+        >>> context = await perceive.scan()
+        >>> print(f"Project: {context.project_id}")
+        >>> print(f"Services: {len(context.services)}")
+    """
+    
+    def __init__(self):
+        """Inicializa el modulo de percepcion."""
+        self._discovery = None
+        self._last_context: Optional[EnvironmentContext] = None
+        self._scan_count = 0
+    
+    @property
+    def discovery(self):
+        """Lazy initialization de GCPDiscovery."""
+        if self._discovery is None:
+            try:
+                from ..core.gcp_discovery import GCPDiscovery
+                self._discovery = GCPDiscovery()
+                logger.info("GCP Discovery initialized")
+            except ImportError as e:
+                logger.warning(f"GCP Discovery not available: {e}")
+                self._discovery = None
+            except Exception as e:
+                logger.error(f"Failed to initialize GCP Discovery: {e}")
+                self._discovery = None
+        return self._discovery
+    
+    async def scan(self) -> EnvironmentContext:
+        """Escanea el entorno GCP completo.
+        
+        Ejecuta discovery de:
+        - Proyecto y credenciales
+        - Servicios habilitados
+        - Recursos por servicio (via plugins)
+        - Estado de memoria
+        
+        Tambien detecta cambios desde el ultimo scan.
+        
+        Returns:
+            EnvironmentContext con toda la informacion
+            
+        Raises:
+            No lanza excepciones - retorna contexto parcial en caso de error
+        """
+        logger.info("[PERCEIVE] Starting environment scan...")
+        self._scan_count += 1
+        
+        # Valores por defecto
+        project_id = "unknown"
+        region = "us-central1"
+        services: List[str] = []
+        resources: Dict[str, Dict[str, Any]] = {}
+        
+        # Intentar discovery
+        if self.discovery is not None:
+            try:
+                # Descubrir proyecto
+                project = self.discovery.discover_project()
+                project_id = project.project_id
+                region = project.region
+                logger.info(f"[PERCEIVE] Project: {project_id}")
+                
+                # Descubrir servicios
+                enabled_services = self.discovery.discover_enabled_services()
+                services = [s.name for s in enabled_services if s.enabled]
+                logger.info(f"[PERCEIVE] Services: {len(services)} enabled")
+                
+                # Descubrir recursos via plugins
+                resources = self.discovery.discover_all_service_resources()
+                logger.info(f"[PERCEIVE] Resources: {len(resources)} services with resources")
+                
+            except Exception as e:
+                logger.error(f"[PERCEIVE] Discovery error: {e}")
+        else:
+            logger.warning("[PERCEIVE] GCP Discovery not available")
+        
+        # Detectar cambios
+        changes = self._detect_changes(project_id, services, resources)
+        
+        # Obtener estado de memoria
+        memory_state = await self._get_memory_state()
+        
+        # Construir contexto
+        context = EnvironmentContext(
+            project_id=project_id,
+            region=region,
+            services=services,
+            resources=resources,
+            changes=changes,
+            memory_state=memory_state,
+            timestamp=datetime.utcnow(),
+        )
+        
+        # Guardar para comparacion futura
+        self._last_context = context
+        
+        logger.info(
+            f"[PERCEIVE] Scan complete: "
+            f"project={project_id}, "
+            f"services={len(services)}, "
+            f"resources={len(resources)}, "
+            f"changes={len(changes)}"
+        )
+        
+        return context
+    
+    def _detect_changes(
+        self,
+        project_id: str,
+        services: List[str],
+        resources: Dict[str, Dict[str, Any]],
+    ) -> List[Dict[str, Any]]:
+        """Detecta cambios desde el ultimo scan.
+        
+        Args:
+            project_id: ID del proyecto actual
+            services: Servicios actuales
+            resources: Recursos actuales
+            
+        Returns:
+            Lista de cambios detectados
+        """
+        if self._last_context is None:
+            return [{"type": "initial_scan", "scan_number": self._scan_count}]
+        
+        changes: List[Dict[str, Any]] = []
+        
+        # Comparar servicios
+        old_services = set(self._last_context.services)
+        new_services = set(services)
+        
+        added_services = new_services - old_services
+        removed_services = old_services - new_services
+        
+        if added_services:
+            changes.append({
+                "type": "services_added",
+                "services": list(added_services),
+            })
+        
+        if removed_services:
+            changes.append({
+                "type": "services_removed",
+                "services": list(removed_services),
+            })
+        
+        # Comparar recursos
+        for service, current_data in resources.items():
+            old_data = self._last_context.resources.get(service, {})
+            old_count = old_data.get("count", 0)
+            new_count = current_data.get("count", 0)
+            
+            if new_count != old_count:
+                change_type = "increased" if new_count > old_count else "decreased"
+                changes.append({
+                    "type": f"resource_count_{change_type}",
+                    "service": service,
+                    "old_count": old_count,
+                    "new_count": new_count,
+                    "delta": new_count - old_count,
+                })
+        
+        # Detectar nuevos servicios con recursos
+        old_resource_services = set(self._last_context.resources.keys())
+        new_resource_services = set(resources.keys())
+        
+        new_with_resources = new_resource_services - old_resource_services
+        if new_with_resources:
+            changes.append({
+                "type": "new_services_with_resources",
+                "services": list(new_with_resources),
+            })
+        
+        return changes
+    
+    async def _get_memory_state(self) -> Dict[str, Any]:
+        """Obtiene estado de memoria desde Firestore.
+        
+        Returns:
+            Estado de memoria o diccionario vacio si no disponible
+        """
+        try:
+            from ..cloud.firestore import FirestoreClient
+            client = FirestoreClient()
+            state = await client.get_genesis_state()
+            return state or {}
+        except ImportError:
+            logger.debug("Firestore client not available")
+            return {"status": "memory_unavailable", "reason": "firestore_not_installed"}
+        except Exception as e:
+            logger.debug(f"Could not get memory state: {e}")
+            return {"status": "memory_error", "error": str(e)}
+    
+    def get_last_context(self) -> Optional[EnvironmentContext]:
+        """Obtiene el ultimo contexto escaneado.
+        
+        Returns:
+            Ultimo contexto o None si no hay
+        """
+        return self._last_context
+{%- endif %}
diff --git a/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/genesis/think.py b/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/genesis/think.py
new file mode 100644
index 0000000..97b0994
--- /dev/null
+++ b/{{cookiecutter.project_name}}/src/{{cookiecutter.package_name}}/genesis/think.py
@@ -0,0 +1,393 @@
+{%- if cookiecutter.use_google_adk == 'y' and cookiecutter.use_google_cloud == 'y' %}
+"""Modulo de Pensamiento - Razonamiento con Gemini.
+
+Este modulo implementa la fase THINK del ciclo GENESIS.
+Utiliza Gemini para razonar sobre el contexto y generar
+planes de accion estructurados.
+"""
+import json
+import logging
+from dataclasses import dataclass, field
+from typing import List, Optional, Dict, Any
+
+logger = logging.getLogger(__name__)
+
+
+@dataclass
+class Action:
+    """Accion individual a ejecutar.
+    
+    Attributes:
+        type: Tipo de accion (generate_agent, generate_plugin, deploy, query, modify_code)
+        target: Objetivo de la accion
+        spec: Especificacion detallada
+        priority: Prioridad (0-10, mayor = mas importante)
+        reasoning: Razon para esta accion
+    """
+    type: str
+    target: str
+    spec: Dict[str, Any] = field(default_factory=dict)
+    priority: int = 0
+    reasoning: str = ""
+    
+    def to_dict(self) -> dict:
+        """Convierte a diccionario."""
+        return {
+            "type": self.type,
+            "target": self.target,
+            "spec": self.spec,
+            "priority": self.priority,
+            "reasoning": self.reasoning,
+        }
+    
+    @classmethod
+    def from_dict(cls, data: dict) -> "Action":
+        """Crea Action desde diccionario."""
+        return cls(
+            type=data.get("type", "unknown"),
+            target=data.get("target", ""),
+            spec=data.get("spec", {}),
+            priority=data.get("priority", 0),
+            reasoning=data.get("reasoning", ""),
+        )
+
+
+@dataclass
+class ActionPlan:
+    """Plan de acciones generado por el modulo Think.
+    
+    Attributes:
+        reasoning: Explicacion del razonamiento
+        actions: Lista de acciones a ejecutar
+        confidence: Confianza en el plan (0-1)
+        context_hash: Hash del contexto usado
+    """
+    reasoning: str = ""
+    actions: List[Action] = field(default_factory=list)
+    confidence: float = 0.0
+    context_hash: str = ""
+    
+    def to_dict(self) -> dict:
+        """Convierte a diccionario."""
+        return {
+            "reasoning": self.reasoning,
+            "actions": [a.to_dict() for a in self.actions],
+            "confidence": self.confidence,
+            "context_hash": self.context_hash,
+        }
+    
+    @classmethod
+    def from_json(cls, data: dict) -> "ActionPlan":
+        """Crea ActionPlan desde respuesta JSON de Gemini."""
+        actions = [
+            Action.from_dict(a)
+            for a in data.get("actions", [])
+        ]
+        
+        return cls(
+            reasoning=data.get("reasoning", ""),
+            actions=actions,
+            confidence=data.get("confidence", 0.5),
+            context_hash=data.get("context_hash", ""),
+        )
+    
+    @classmethod
+    def empty(cls) -> "ActionPlan":
+        """Crea plan vacio para casos de error."""
+        return cls(
+            reasoning="No plan generated due to error",
+            actions=[],
+            confidence=0.0,
+        )
+    
+    def get_actions_by_priority(self) -> List[Action]:
+        """Retorna acciones ordenadas por prioridad (mayor primero)."""
+        return sorted(self.actions, key=lambda a: -a.priority)
+
+
+class ThinkModule:
+    """Modulo de razonamiento con Gemini.
+    
+    Utiliza GoogleADKAgent para razonar sobre el contexto
+    y generar planes de accion estructurados.
+    
+    Example:
+        >>> think = ThinkModule()
+        >>> plan = await think.reason(context)
+        >>> for action in plan.actions:
+        ...     print(f"{action.type}: {action.target}")
+    """
+    
+    # System prompt para razonamiento
+    SYSTEM_PROMPT = '''Eres GENESIS, un sistema autopoietico que vive en Google Cloud.
+
+Tu proposito es analizar el entorno GCP y decidir acciones para:
+1. Crear agentes especializados para servicios descubiertos
+2. Generar plugins para nuevos servicios sin soporte
+3. Optimizar el funcionamiento del sistema
+4. Responder a tareas del usuario
+
+REGLAS:
+- Siempre responde en JSON valido
+- Prioriza acciones que mejoren el sistema a largo plazo
+- Genera codigo production-ready cuando sea necesario
+- Nunca inventes recursos que no existen en el contexto
+
+TIPOS DE ACCIONES DISPONIBLES:
+- generate_agent: Crear nuevo agente para un servicio GCP
+- generate_plugin: Crear plugin para discovery de un servicio
+- deploy: Desplegar cambios a Cloud Run
+- query: Consultar datos/recursos
+- modify_code: Modificar codigo existente del sistema
+
+FORMATO DE RESPUESTA (JSON):
+{
+    "reasoning": "Explicacion detallada del razonamiento",
+    "confidence": 0.0-1.0,
+    "actions": [
+        {
+            "type": "generate_agent|generate_plugin|deploy|query|modify_code",
+            "target": "nombre del servicio/recurso",
+            "spec": {
+                "description": "que debe hacer",
+                "requirements": ["lista", "de", "requerimientos"]
+            },
+            "priority": 0-10,
+            "reasoning": "por que esta accion"
+        }
+    ]
+}
+
+Si no hay acciones necesarias, retorna actions como lista vacia con reasoning explicando por que.'''
+
+    CODE_GENERATION_PROMPT = '''Genera codigo Python production-ready para: {spec}
+
+REGLAS ESTRICTAS:
+1. Type hints completos en todas las funciones
+2. Docstrings en formato Google
+3. Manejo de errores robusto con try/except especificos
+4. Logging apropiado usando logging module
+5. Compatible con async/await donde sea apropiado
+6. Sin dependencias externas innecesarias
+7. Codigo limpio siguiendo PEP8
+
+ESTRUCTURA REQUERIDA:
+- Imports al inicio
+- Constantes despues de imports
+- Clases principales
+- Funciones auxiliares
+- Bloque if __name__ == "__main__" si es ejecutable
+
+Responde SOLO con el codigo Python, sin explicaciones ni markdown.'''
+
+    def __init__(self):
+        """Inicializa el modulo de pensamiento."""
+        self._agent = None
+        logger.info("ThinkModule initialized")
+    
+    @property
+    def agent(self):
+        """Lazy initialization del agente Gemini."""
+        if self._agent is None:
+            try:
+                from ..agents.adk import GoogleADKAgent, ADKConfig
+                config = ADKConfig(
+                    model="gemini-2.0-flash-exp",
+                    temperature=0.7,
+                    max_tokens=8192,
+                    system_instruction=self.SYSTEM_PROMPT,
+                )
+                self._agent = GoogleADKAgent(config)
+                logger.info("Gemini agent initialized for ThinkModule")
+            except Exception as e:
+                logger.error(f"Failed to initialize Gemini agent: {e}")
+                raise
+        return self._agent
+    
+    async def reason(self, context) -> ActionPlan:
+        """Razona sobre el contexto y genera plan de acciones.
+        
+        Args:
+            context: EnvironmentContext con informacion del entorno
+            
+        Returns:
+            ActionPlan con acciones a ejecutar
+        """
+        logger.info("[THINK] Starting reasoning process...")
+        
+        try:
+            # Generar prompt desde contexto
+            prompt = context.to_prompt()
+            
+            # Llamar a Gemini
+            logger.debug("[THINK] Calling Gemini for reasoning...")
+            response = await self.agent.run(prompt)
+            
+            # Parsear respuesta JSON
+            plan = self._parse_response(response, context.hash())
+            
+            logger.info(
+                f"[THINK] Reasoning complete: "
+                f"{len(plan.actions)} actions, "
+                f"confidence={plan.confidence:.2f}"
+            )
+            
+            return plan
+            
+        except json.JSONDecodeError as e:
+            logger.error(f"[THINK] Failed to parse Gemini response as JSON: {e}")
+            return ActionPlan(
+                reasoning=f"Failed to parse response: {str(e)}",
+                actions=[],
+                confidence=0.0,
+            )
+        except Exception as e:
+            logger.error(f"[THINK] Reasoning failed: {e}")
+            return ActionPlan.empty()
+    
+    async def generate_code(self, spec: str) -> str:
+        """Genera codigo Python basado en especificacion.
+        
+        Args:
+            spec: Especificacion de que codigo generar
+            
+        Returns:
+            Codigo Python generado
+            
+        Raises:
+            ValueError: Si la generacion falla
+        """
+        logger.info(f"[THINK] Generating code for: {spec[:50]}...")
+        
+        prompt = self.CODE_GENERATION_PROMPT.format(spec=spec)
+        
+        try:
+            # Usar agente sin system prompt de razonamiento
+            from ..agents.adk import GoogleADKAgent, ADKConfig
+            code_config = ADKConfig(
+                model="gemini-2.0-flash-exp",
+                temperature=0.3,  # Menor temperatura para codigo
+                max_tokens=8192,
+            )
+            code_agent = GoogleADKAgent(code_config)
+            
+            code = await code_agent.run(prompt)
+            
+            # Limpiar respuesta
+            code = self._clean_code(code)
+            
+            # Validar sintaxis
+            self._validate_syntax(code)
+            
+            logger.info("[THINK] Code generation successful")
+            return code
+            
+        except SyntaxError as e:
+            logger.error(f"[THINK] Generated code has syntax error: {e}")
+            raise ValueError(f"Generated code is invalid: {e}")
+        except Exception as e:
+            logger.error(f"[THINK] Code generation failed: {e}")
+            raise
+    
+    def _parse_response(self, response: str, context_hash: str) -> ActionPlan:
+        """Parsea respuesta de Gemini a ActionPlan.
+        
+        Args:
+            response: Respuesta raw de Gemini
+            context_hash: Hash del contexto para tracking
+            
+        Returns:
+            ActionPlan parseado
+        """
+        # Intentar extraer JSON de la respuesta
+        json_str = self._extract_json(response)
+        
+        if json_str:
+            data = json.loads(json_str)
+            plan = ActionPlan.from_json(data)
+            plan.context_hash = context_hash
+            return plan
+        
+        # Si no hay JSON, crear plan con el reasoning como texto
+        return ActionPlan(
+            reasoning=response,
+            actions=[],
+            confidence=0.1,
+            context_hash=context_hash,
+        )
+    
+    def _extract_json(self, text: str) -> Optional[str]:
+        """Extrae JSON de texto que puede contener markdown.
+        
+        Args:
+            text: Texto que puede contener JSON
+            
+        Returns:
+            String JSON o None
+        """
+        # Intentar parsear directamente
+        try:
+            json.loads(text)
+            return text
+        except json.JSONDecodeError:
+            pass
+        
+        # Buscar bloques de codigo JSON
+        import re
+        
+        # Patron para ```json ... ```
+        json_block = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', text, re.DOTALL)
+        if json_block:
+            return json_block.group(1)
+        
+        # Buscar objeto JSON directo
+        json_obj = re.search(r'\{[^{}]*"reasoning"[^{}]*\}', text, re.DOTALL)
+        if json_obj:
+            try:
+                json.loads(json_obj.group())
+                return json_obj.group()
+            except json.JSONDecodeError:
+                pass
+        
+        # Buscar JSON con acciones anidadas
+        json_full = re.search(r'\{.*"actions".*\}', text, re.DOTALL)
+        if json_full:
+            try:
+                json.loads(json_full.group())
+                return json_full.group()
+            except json.JSONDecodeError:
+                pass
+        
+        return None
+    
+    def _clean_code(self, code: str) -> str:
+        """Limpia codigo generado de markdown y extras.
+        
+        Args:
+            code: Codigo potencialmente con markdown
+            
+        Returns:
+            Codigo limpio
+        """
+        # Remover bloques de codigo markdown
+        import re
+        
+        # Patron para ```python ... ```
+        code_block = re.search(r'```(?:python)?\s*(.*?)\s*```', code, re.DOTALL)
+        if code_block:
+            return code_block.group(1).strip()
+        
+        return code.strip()
+    
+    def _validate_syntax(self, code: str) -> None:
+        """Valida sintaxis del codigo Python.
+        
+        Args:
+            code: Codigo a validar
+            
+        Raises:
+            SyntaxError: Si el codigo es invalido
+        """
+        import ast
+        ast.parse(code)
+{%- endif %}
diff --git a/{{cookiecutter.project_name}}/tests/agents/test_factory.py b/{{cookiecutter.project_name}}/tests/agents/test_factory.py
new file mode 100644
index 0000000..a0936fd
--- /dev/null
+++ b/{{cookiecutter.project_name}}/tests/agents/test_factory.py
@@ -0,0 +1,163 @@
+{%- if cookiecutter.use_google_adk == 'y' %}
+"""Tests for Agent Factory."""
+import pytest
+from unittest.mock import MagicMock, patch
+
+
+class TestAgentSpec:
+    """Tests for AgentSpec dataclass."""
+    
+    def test_agent_spec_defaults(self):
+        """Test AgentSpec default values."""
+        from {{cookiecutter.package_name}}.agents.factory import AgentSpec, AgentType
+        
+        spec = AgentSpec(
+            name="test-agent",
+            agent_type=AgentType.RESEARCH,
+        )
+        
+        assert spec.name == "test-agent"
+        assert spec.model == "gemini-2.0-flash-exp"
+        assert spec.temperature == 0.7
+        assert spec.capabilities == []
+
+
+class TestAgentType:
+    """Tests for AgentType enum."""
+    
+    def test_agent_types_exist(self):
+        """Test all expected agent types exist."""
+        from {{cookiecutter.package_name}}.agents.factory import AgentType
+        
+        assert AgentType.RESEARCH.value == "research"
+        assert AgentType.ANALYSIS.value == "analysis"
+        assert AgentType.WRITER.value == "writer"
+        assert AgentType.CODE.value == "code"
+        assert AgentType.DATA.value == "data"
+
+
+class TestAgentPrompts:
+    """Tests for agent system prompts."""
+    
+    def test_prompts_contain_placeholder(self):
+        """Test prompts have service placeholder."""
+        from {{cookiecutter.package_name}}.agents.factory import AGENT_PROMPTS, AgentType
+        
+        for agent_type, prompt in AGENT_PROMPTS.items():
+            assert "{target_service}" in prompt, f"{agent_type} missing placeholder"
+
+
+class TestServiceAgentMapping:
+    """Tests for service to agent mapping."""
+    
+    def test_bigquery_mapping(self):
+        """Test BigQuery service mapping."""
+        from {{cookiecutter.package_name}}.agents.factory import (
+            SERVICE_AGENT_MAPPING,
+            AgentType,
+        )
+        
+        mapping = SERVICE_AGENT_MAPPING.get("bigquery", [])
+        
+        assert AgentType.DATA in mapping
+        assert AgentType.ANALYSIS in mapping
+
+
+class TestAgentFactory:
+    """Tests for AgentFactory."""
+    
+    @pytest.fixture
+    def mock_adk_agent(self):
+        """Mock GoogleADKAgent."""
+        with patch("{{cookiecutter.package_name}}.agents.factory.GoogleADKAgent") as mock:
+            mock.return_value = MagicMock()
+            yield mock
+    
+    @pytest.fixture
+    def mock_adk_config(self):
+        """Mock ADKConfig."""
+        with patch("{{cookiecutter.package_name}}.agents.factory.ADKConfig") as mock:
+            yield mock
+    
+    def test_factory_initialization(self):
+        """Test factory initializes correctly."""
+        from {{cookiecutter.package_name}}.agents.factory import AgentFactory
+        
+        factory = AgentFactory(api_key="test-key")
+        
+        assert factory._api_key == "test-key"
+        assert factory._agents == {}
+    
+    def test_factory_create_agent(self, mock_adk_agent, mock_adk_config):
+        """Test creating an agent."""
+        from {{cookiecutter.package_name}}.agents.factory import (
+            AgentFactory,
+            AgentType,
+        )
+        
+        factory = AgentFactory(api_key="test-key")
+        agent = factory.create("test-agent", AgentType.RESEARCH)
+        
+        assert agent is not None
+        assert "test-agent" in factory._agents
+    
+    def test_factory_caches_agents(self, mock_adk_agent, mock_adk_config):
+        """Test factory caches created agents."""
+        from {{cookiecutter.package_name}}.agents.factory import (
+            AgentFactory,
+            AgentType,
+        )
+        
+        factory = AgentFactory(api_key="test-key")
+        
+        agent1 = factory.create("cached-agent", AgentType.RESEARCH)
+        agent2 = factory.create("cached-agent", AgentType.RESEARCH)
+        
+        assert agent1 is agent2
+    
+    def test_factory_list_agents(self, mock_adk_agent, mock_adk_config):
+        """Test listing created agents."""
+        from {{cookiecutter.package_name}}.agents.factory import (
+            AgentFactory,
+            AgentType,
+        )
+        
+        factory = AgentFactory(api_key="test-key")
+        factory.create("agent1", AgentType.RESEARCH)
+        factory.create("agent2", AgentType.ANALYSIS)
+        
+        agents = factory.list_agents()
+        
+        assert len(agents) == 2
+        assert "agent1" in agents
+        assert "agent2" in agents
+    
+    def test_factory_clear(self, mock_adk_agent, mock_adk_config):
+        """Test clearing factory cache."""
+        from {{cookiecutter.package_name}}.agents.factory import (
+            AgentFactory,
+            AgentType,
+        )
+        
+        factory = AgentFactory(api_key="test-key")
+        factory.create("agent", AgentType.RESEARCH)
+        
+        factory.clear()
+        
+        assert factory.list_agents() == []
+    
+    def test_factory_create_workers(self, mock_adk_agent, mock_adk_config):
+        """Test creating worker agents."""
+        from {{cookiecutter.package_name}}.agents.factory import (
+            AgentFactory,
+            AgentType,
+        )
+        
+        factory = AgentFactory(api_key="test-key")
+        workers = factory.create_workers()
+        
+        assert "research" in workers
+        assert "analysis" in workers
+        assert "writer" in workers
+        assert "code" in workers
+{%- endif %}
diff --git a/{{cookiecutter.project_name}}/tests/cloud/__init__.py b/{{cookiecutter.project_name}}/tests/cloud/__init__.py
new file mode 100644
index 0000000..923c7b6
--- /dev/null
+++ b/{{cookiecutter.project_name}}/tests/cloud/__init__.py
@@ -0,0 +1,3 @@
+{%- if cookiecutter.use_google_cloud == 'y' %}
+"""Tests for cloud infrastructure modules."""
+{%- endif %}
diff --git a/{{cookiecutter.project_name}}/tests/cloud/test_cloud_run.py b/{{cookiecutter.project_name}}/tests/cloud/test_cloud_run.py
new file mode 100644
index 0000000..fcca164
--- /dev/null
+++ b/{{cookiecutter.project_name}}/tests/cloud/test_cloud_run.py
@@ -0,0 +1,83 @@
+{%- if cookiecutter.use_google_cloud == 'y' %}
+"""Tests for Cloud Run deployer."""
+import pytest
+from unittest.mock import MagicMock, patch, AsyncMock
+
+
+class TestDeploymentConfig:
+    """Tests for DeploymentConfig."""
+    
+    def test_default_config(self):
+        """Test default configuration values."""
+        from {{cookiecutter.package_name}}.cloud.run import DeploymentConfig
+        
+        config = DeploymentConfig()
+        
+        assert config.service_name == "genesis"
+        assert config.region == "us-central1"
+        assert config.memory == "256Mi"
+        assert config.min_instances == 0
+        assert config.max_instances == 1
+        assert config.allow_unauthenticated is False
+    
+    def test_custom_config(self):
+        """Test custom configuration."""
+        from {{cookiecutter.package_name}}.cloud.run import DeploymentConfig
+        
+        config = DeploymentConfig(
+            service_name="custom-service",
+            region="europe-west1",
+            memory="512Mi",
+            max_instances=10,
+            env_vars={"KEY": "value"},
+        )
+        
+        assert config.service_name == "custom-service"
+        assert config.region == "europe-west1"
+        assert config.max_instances == 10
+        assert config.env_vars["KEY"] == "value"
+    
+    def test_env_vars_default_to_dict(self):
+        """Test env_vars defaults to empty dict."""
+        from {{cookiecutter.package_name}}.cloud.run import DeploymentConfig
+        
+        config = DeploymentConfig()
+        
+        assert config.env_vars == {}
+
+
+class TestCloudRunDeployer:
+    """Tests for CloudRunDeployer."""
+    
+    def test_initialization(self):
+        """Test deployer initialization."""
+        with patch.dict("os.environ", {"GOOGLE_CLOUD_PROJECT": "test-project"}):
+            from {{cookiecutter.package_name}}.cloud.run import CloudRunDeployer
+            
+            deployer = CloudRunDeployer()
+            
+            assert deployer._project_id == "test-project"
+            assert deployer.config is not None
+    
+    def test_initialization_with_config(self):
+        """Test deployer with custom config."""
+        from {{cookiecutter.package_name}}.cloud.run import (
+            CloudRunDeployer,
+            DeploymentConfig,
+        )
+        
+        config = DeploymentConfig(service_name="custom")
+        deployer = CloudRunDeployer(config=config)
+        
+        assert deployer.config.service_name == "custom"
+    
+    @pytest.mark.asyncio
+    async def test_deploy_requires_image(self):
+        """Test deploy fails without image."""
+        from {{cookiecutter.package_name}}.cloud.run import CloudRunDeployer
+        
+        deployer = CloudRunDeployer(project_id="test")
+        
+        with pytest.raises(ValueError, match="No image"):
+            await deployer.deploy()
+{%- endif %}
diff --git a/{{cookiecutter.project_name}}/tests/cloud/test_firestore.py b/{{cookiecutter.project_name}}/tests/cloud/test_firestore.py
new file mode 100644
index 0000000..ee308b4
--- /dev/null
+++ b/{{cookiecutter.project_name}}/tests/cloud/test_firestore.py
@@ -0,0 +1,48 @@
+{%- if cookiecutter.use_google_cloud == 'y' %}
+"""Tests for Firestore client."""
+import pytest
+from unittest.mock import MagicMock, patch, AsyncMock
+
+
+class TestFirestoreClient:
+    """Tests for FirestoreClient."""
+    
+    def test_initialization_without_project(self):
+        """Test client initializes without explicit project."""
+        with patch.dict("os.environ", {}, clear=True):
+            from {{cookiecutter.package_name}}.cloud.firestore import FirestoreClient
+            
+            client = FirestoreClient()
+            
+            assert client._project_id is None
+            assert client._client is None
+    
+    def test_initialization_with_project(self):
+        """Test client initializes with project."""
+        from {{cookiecutter.package_name}}.cloud.firestore import FirestoreClient
+        
+        client = FirestoreClient(project_id="test-project")
+        
+        assert client._project_id == "test-project"
+    
+    def test_initialization_from_env(self):
+        """Test client uses environment variable."""
+        with patch.dict("os.environ", {"GOOGLE_CLOUD_PROJECT": "env-project"}):
+            from {{cookiecutter.package_name}}.cloud.firestore import FirestoreClient
+            
+            client = FirestoreClient()
+            
+            assert client._project_id == "env-project"
+    
+    @pytest.mark.asyncio
+    async def test_close(self):
+        """Test closing client."""
+        from {{cookiecutter.package_name}}.cloud.firestore import FirestoreClient
+        
+        client = FirestoreClient(project_id="test")
+        
+        await client.close()
+        
+        assert client._client is None
+        assert client._initialized is False
+{%- endif %}
diff --git a/{{cookiecutter.project_name}}/tests/cloud/test_pubsub.py b/{{cookiecutter.project_name}}/tests/cloud/test_pubsub.py
new file mode 100644
index 0000000..66f9427
--- /dev/null
+++ b/{{cookiecutter.project_name}}/tests/cloud/test_pubsub.py
@@ -0,0 +1,76 @@
+{%- if cookiecutter.use_google_cloud == 'y' %}
+"""Tests for Pub/Sub client."""
+import pytest
+from unittest.mock import MagicMock, patch
+import json
+
+
+class TestPubSubMessage:
+    """Tests for PubSubMessage."""
+    
+    def test_message_to_bytes(self):
+        """Test message serialization."""
+        from {{cookiecutter.package_name}}.cloud.pubsub import PubSubMessage
+        
+        msg = PubSubMessage(
+            data={"key": "value", "number": 123},
+            attributes={"source": "test"},
+        )
+        
+        raw = msg.to_bytes()
+        
+        assert isinstance(raw, bytes)
+        parsed = json.loads(raw.decode("utf-8"))
+        assert parsed["key"] == "value"
+        assert parsed["number"] == 123
+    
+    def test_message_from_bytes(self):
+        """Test message deserialization."""
+        from {{cookiecutter.package_name}}.cloud.pubsub import PubSubMessage
+        
+        data = json.dumps({"key": "value"}).encode("utf-8")
+        
+        msg = PubSubMessage.from_bytes(data)
+        
+        assert msg.data["key"] == "value"
+    
+    def test_message_attributes_default(self):
+        """Test message has default attributes."""
+        from {{cookiecutter.package_name}}.cloud.pubsub import PubSubMessage
+        
+        msg = PubSubMessage(data={"test": True})
+        
+        assert msg.attributes == {}
+
+
+class TestPubSubClient:
+    """Tests for PubSubClient."""
+    
+    def test_topic_path(self):
+        """Test topic path construction."""
+        with patch.dict("os.environ", {"GOOGLE_CLOUD_PROJECT": "test-project"}):
+            from {{cookiecutter.package_name}}.cloud.pubsub import PubSubClient
+            
+            client = PubSubClient()
+            
+            path = client._topic_path("my-topic")
+            
+            assert path == "projects/test-project/topics/my-topic"
+    
+    def test_subscription_path(self):
+        """Test subscription path construction."""
+        with patch.dict("os.environ", {"GOOGLE_CLOUD_PROJECT": "test-project"}):
+            from {{cookiecutter.package_name}}.cloud.pubsub import PubSubClient
+            
+            client = PubSubClient()
+            
+            path = client._subscription_path("my-sub")
+            
+            assert path == "projects/test-project/subscriptions/my-sub"
+    
+    def test_genesis_prefix(self):
+        """Test GENESIS topic prefix."""
+        from {{cookiecutter.package_name}}.cloud.pubsub import PubSubClient
+        
+        assert PubSubClient.GENESIS_PREFIX == "genesis-"
+{%- endif %}
diff --git a/{{cookiecutter.project_name}}/tests/genesis/__init__.py b/{{cookiecutter.project_name}}/tests/genesis/__init__.py
new file mode 100644
index 0000000..85a21cd
--- /dev/null
+++ b/{{cookiecutter.project_name}}/tests/genesis/__init__.py
@@ -0,0 +1,3 @@
+{%- if cookiecutter.use_google_adk == 'y' and cookiecutter.use_google_cloud == 'y' %}
+"""Tests for GENESIS autopoietic system."""
+{%- endif %}
diff --git a/{{cookiecutter.project_name}}/tests/genesis/test_genesis_act.py b/{{cookiecutter.project_name}}/tests/genesis/test_genesis_act.py
new file mode 100644
index 0000000..4feab9d
--- /dev/null
+++ b/{{cookiecutter.project_name}}/tests/genesis/test_genesis_act.py
@@ -0,0 +1,108 @@
+{%- if cookiecutter.use_google_adk == 'y' and cookiecutter.use_google_cloud == 'y' %}
+"""Tests for GENESIS Act module."""
+import pytest
+from unittest.mock import MagicMock, patch, AsyncMock
+
+
+class TestActionResult:
+    """Tests for ActionResult dataclass."""
+    
+    def test_action_result_to_dict(self):
+        """Test ActionResult serialization."""
+        from {{cookiecutter.package_name}}.genesis.act import ActionResult
+        
+        result = ActionResult(
+            actions=["action1:target1", "action2:target2"],
+            success=True,
+            outputs=["output1", "output2"],
+            errors=[],
+        )
+        
+        data = result.to_dict()
+        
+        assert len(data["actions"]) == 2
+        assert data["success"] is True
+        assert "timestamp" in data
+    
+    def test_action_result_empty(self):
+        """Test empty result creation."""
+        from {{cookiecutter.package_name}}.genesis.act import ActionResult
+        
+        result = ActionResult.empty()
+        
+        assert result.success is False
+        assert len(result.errors) > 0
+
+
+class TestActModule:
+    """Tests for ActModule."""
+    
+    def test_to_class_name_simple(self):
+        """Test class name conversion - simple."""
+        from {{cookiecutter.package_name}}.genesis.act import ActModule
+        
+        module = ActModule()
+        
+        assert module._to_class_name("bigquery") == "Bigquery"
+        assert module._to_class_name("cloud-run") == "CloudRun"
+    
+    def test_to_class_name_with_prefix(self):
+        """Test class name conversion with GCP prefixes."""
+        from {{cookiecutter.package_name}}.genesis.act import ActModule
+        
+        module = ActModule()
+        
+        assert module._to_class_name("google-cloud-storage") == "Storage"
+        assert module._to_class_name("gcp-compute") == "Compute"
+    
+    def test_to_class_name_with_dots(self):
+        """Test class name conversion with dots."""
+        from {{cookiecutter.package_name}}.genesis.act import ActModule
+        
+        module = ActModule()
+        
+        assert module._to_class_name("vertex.ai") == "VertexAi"
+    
+    @pytest.mark.asyncio
+    async def test_execute_unknown_action_type(self):
+        """Test executing unknown action type fails gracefully."""
+        from {{cookiecutter.package_name}}.genesis.act import ActModule
+        from {{cookiecutter.package_name}}.genesis.think import Action, ActionPlan
+        
+        module = ActModule()
+        
+        plan = ActionPlan(
+            reasoning="Test",
+            actions=[
+                Action(type="unknown_type", target="test", priority=1),
+            ],
+        )
+        
+        result = await module.execute(plan)
+        
+        assert not result.success
+        assert len(result.errors) > 0
+        assert "unknown" in result.errors[0].lower()
+    
+    @pytest.mark.asyncio
+    async def test_execute_empty_plan(self):
+        """Test executing empty plan."""
+        from {{cookiecutter.package_name}}.genesis.act import ActModule
+        from {{cookiecutter.package_name}}.genesis.think import ActionPlan
+        
+        module = ActModule()
+        plan = ActionPlan.empty()
+        
+        result = await module.execute(plan)
+        
+        # Empty plan succeeds but with no actions
+        assert len(result.actions) == 0
+    
+    def test_get_generated_files_initially_empty(self):
+        """Test generated files list is initially empty."""
+        from {{cookiecutter.package_name}}.genesis.act import ActModule
+        
+        module = ActModule()
+        
+        assert module.get_generated_files() == []
+{%- endif %}
diff --git a/{{cookiecutter.project_name}}/tests/genesis/test_genesis_core.py b/{{cookiecutter.project_name}}/tests/genesis/test_genesis_core.py
new file mode 100644
index 0000000..3ddc35a
--- /dev/null
+++ b/{{cookiecutter.project_name}}/tests/genesis/test_genesis_core.py
@@ -0,0 +1,251 @@
+{%- if cookiecutter.use_google_adk == 'y' and cookiecutter.use_google_cloud == 'y' %}
+"""Tests for GENESIS Core module.
+
+These tests verify the GenesisCore system behavior using mocks
+to avoid requiring actual GCP credentials during testing.
+"""
+import pytest
+from unittest.mock import AsyncMock, MagicMock, patch
+from datetime import datetime
+
+
+@pytest.fixture
+def mock_perceive():
+    """Mock PerceiveModule."""
+    with patch("{{cookiecutter.package_name}}.genesis.core.PerceiveModule") as mock:
+        module = MagicMock()
+        
+        # Create mock context
+        context = MagicMock()
+        context.hash.return_value = "test_hash_123"
+        context.user_task = None
+        context.to_prompt.return_value = "Test context prompt"
+        
+        module.scan = AsyncMock(return_value=context)
+        mock.return_value = module
+        
+        yield mock
+
+
+@pytest.fixture
+def mock_think():
+    """Mock ThinkModule."""
+    with patch("{{cookiecutter.package_name}}.genesis.core.ThinkModule") as mock:
+        module = MagicMock()
+        
+        # Create mock plan
+        plan = MagicMock()
+        plan.actions = []
+        plan.reasoning = "Test reasoning"
+        plan.get_actions_by_priority.return_value = []
+        
+        module.reason = AsyncMock(return_value=plan)
+        module.generate_code = AsyncMock(return_value="# Generated code")
+        mock.return_value = module
+        
+        yield mock
+
+
+@pytest.fixture
+def mock_act():
+    """Mock ActModule."""
+    with patch("{{cookiecutter.package_name}}.genesis.core.ActModule") as mock:
+        module = MagicMock()
+        
+        # Create mock result
+        result = MagicMock()
+        result.success = True
+        result.actions = ["test:action"]
+        result.errors = []
+        result.to_dict.return_value = {
+            "actions": ["test:action"],
+            "success": True,
+            "outputs": [],
+            "errors": [],
+        }
+        
+        module.execute = AsyncMock(return_value=result)
+        mock.return_value = module
+        
+        yield mock
+
+
+@pytest.fixture
+def mock_memory():
+    """Mock MemoryModule."""
+    with patch("{{cookiecutter.package_name}}.genesis.core.MemoryModule") as mock:
+        module = MagicMock()
+        module.store_cycle = AsyncMock()
+        mock.return_value = module
+        yield mock
+
+
+@pytest.fixture
+def mock_evolve():
+    """Mock EvolveModule."""
+    with patch("{{cookiecutter.package_name}}.genesis.core.EvolveModule") as mock:
+        module = MagicMock()
+        module.improve = AsyncMock()
+        mock.return_value = module
+        yield mock
+
+
+@pytest.mark.asyncio
+async def test_genesis_core_initialization(
+    mock_perceive, mock_think, mock_act, mock_memory, mock_evolve
+):
+    """Test GenesisCore initializes correctly."""
+    from {{cookiecutter.package_name}}.genesis import GenesisCore
+    
+    core = GenesisCore()
+    
+    assert core.perceive is not None
+    assert core.think is not None
+    assert core.act is not None
+    assert core.memory is not None
+    assert core.evolve is not None
+    assert core._cycle_count == 0
+
+
+@pytest.mark.asyncio
+async def test_genesis_core_run_cycle(
+    mock_perceive, mock_think, mock_act, mock_memory, mock_evolve
+):
+    """Test running a single cycle."""
+    from {{cookiecutter.package_name}}.genesis import GenesisCore
+    
+    core = GenesisCore()
+    result = await core.run_cycle()
+    
+    # Verify phases were called
+    core.perceive.scan.assert_called_once()
+    core.think.reason.assert_called_once()
+    core.act.execute.assert_called_once()
+    core.memory.store_cycle.assert_called_once()
+    
+    # Check result
+    assert result is not None
+    assert result.cycle_id.startswith("cycle_")
+    assert isinstance(result.timestamp, datetime)
+    assert result.success is True
+    assert result.duration_ms > 0
+
+
+@pytest.mark.asyncio
+async def test_genesis_core_run_cycle_with_task(
+    mock_perceive, mock_think, mock_act, mock_memory, mock_evolve
+):
+    """Test running a cycle with specific task."""
+    from {{cookiecutter.package_name}}.genesis import GenesisCore
+    
+    core = GenesisCore()
+    result = await core.run_cycle(task="Test specific task")
+    
+    # Verify task was set in context
+    core.perceive.scan.assert_called_once()
+    context = await core.perceive.scan()
+    
+    assert result.success is True
+
+
+@pytest.mark.asyncio
+async def test_genesis_core_run_cycle_handles_errors(
+    mock_perceive, mock_think, mock_act, mock_memory, mock_evolve
+):
+    """Test cycle handles errors gracefully."""
+    from {{cookiecutter.package_name}}.genesis import GenesisCore
+    
+    # Make think module fail
+    mock_think.return_value.reason = AsyncMock(
+        side_effect=Exception("Think failed")
+    )
+    
+    core = GenesisCore()
+    result = await core.run_cycle()
+    
+    # Should still complete but with errors
+    assert result is not None
+    assert len(result.errors) > 0
+    assert "think" in result.errors[0].lower()
+
+
+@pytest.mark.asyncio
+async def test_genesis_core_evolution(
+    mock_perceive, mock_think, mock_act, mock_memory, mock_evolve
+):
+    """Test evolution is triggered at correct interval."""
+    from {{cookiecutter.package_name}}.genesis import GenesisCore
+    
+    # Set low threshold for testing
+    core = GenesisCore(evolution_threshold=2)
+    
+    # Run 2 cycles (evolution should trigger on 2nd)
+    await core.run_cycle()
+    assert not (await core.run_cycle()).evolved  # First cycle
+    
+    # Reset and run to threshold
+    core._cycle_count = 1
+    result = await core.run_cycle()  # Should trigger on cycle 2
+    
+    # Evolution should have been triggered
+    assert core._cycle_count == 2
+
+
+@pytest.mark.asyncio
+async def test_genesis_core_force_evolve(
+    mock_perceive, mock_think, mock_act, mock_memory, mock_evolve
+):
+    """Test forcing evolution."""
+    from {{cookiecutter.package_name}}.genesis import GenesisCore
+    
+    core = GenesisCore()
+    success = await core.force_evolve()
+    
+    assert success is True
+    core.evolve.improve.assert_called_once()
+
+
+@pytest.mark.asyncio
+async def test_genesis_core_get_status(
+    mock_perceive, mock_think, mock_act, mock_memory, mock_evolve
+):
+    """Test getting system status."""
+    from {{cookiecutter.package_name}}.genesis import GenesisCore
+    
+    core = GenesisCore()
+    await core.run_cycle()
+    
+    status = core.get_status()
+    
+    assert status["status"] == "running"
+    assert "uptime_seconds" in status
+    assert status["cycles_completed"] == 1
+    assert "next_evolution_in" in status
+
+
+class TestCycleResult:
+    """Tests for CycleResult dataclass."""
+    
+    def test_cycle_result_to_dict(self):
+        """Test CycleResult serialization."""
+        from {{cookiecutter.package_name}}.genesis.core import CycleResult
+        
+        result = CycleResult(
+            cycle_id="test_123",
+            timestamp=datetime(2024, 1, 1, 12, 0, 0),
+            context_hash="hash123",
+            plan_summary="Test plan",
+            actions_taken=["action1", "action2"],
+            success=True,
+            duration_ms=100.5,
+            evolved=False,
+            errors=[],
+        )
+        
+        data = result.to_dict()
+        
+        assert data["cycle_id"] == "test_123"
+        assert data["success"] is True
+        assert len(data["actions_taken"]) == 2
+        assert data["duration_ms"] == 100.5
+{%- endif %}
diff --git a/{{cookiecutter.project_name}}/tests/genesis/test_genesis_memory.py b/{{cookiecutter.project_name}}/tests/genesis/test_genesis_memory.py
new file mode 100644
index 0000000..859f92d
--- /dev/null
+++ b/{{cookiecutter.project_name}}/tests/genesis/test_genesis_memory.py
@@ -0,0 +1,123 @@
+{%- if cookiecutter.use_google_adk == 'y' and cookiecutter.use_google_cloud == 'y' %}
+"""Tests for GENESIS Memory module."""
+import pytest
+from unittest.mock import MagicMock, patch, AsyncMock
+
+
+class TestMemoryState:
+    """Tests for MemoryState dataclass."""
+    
+    def test_memory_state_to_dict(self):
+        """Test MemoryState serialization."""
+        from {{cookiecutter.package_name}}.genesis.memory import MemoryState
+        
+        state = MemoryState(
+            total_cycles=100,
+            success_rate=0.95,
+            agents_generated=5,
+            plugins_generated=3,
+        )
+        
+        data = state.to_dict()
+        
+        assert data["total_cycles"] == 100
+        assert data["success_rate"] == 0.95
+        assert data["agents_generated"] == 5
+
+
+class TestMemoryModule:
+    """Tests for MemoryModule."""
+    
+    @pytest.mark.asyncio
+    async def test_local_cache_fallback(self):
+        """Test local cache is used when Firestore unavailable."""
+        from {{cookiecutter.package_name}}.genesis.memory import MemoryModule
+        
+        module = MemoryModule()
+        module._use_local = True
+        
+        # Should use local cache
+        state = await module.get_state()
+        
+        assert state is not None
+        assert state.total_cycles == 0
+    
+    @pytest.mark.asyncio
+    async def test_store_cycle_locally(self):
+        """Test storing cycle in local cache."""
+        from {{cookiecutter.package_name}}.genesis.memory import MemoryModule
+        
+        module = MemoryModule()
+        module._use_local = True
+        
+        # Create mock objects
+        context = MagicMock()
+        context.hash.return_value = "test_hash"
+        context.project_id = "test"
+        context.services = []
+        context.resources = {}
+        context.changes = []
+        context.user_task = None
+        
+        plan = MagicMock()
+        plan.reasoning = "Test"
+        plan.actions = []
+        plan.confidence = 0.5
+        
+        result = MagicMock()
+        result.to_dict.return_value = {"success": True, "actions": []}
+        
+        # Store cycle
+        await module.store_cycle(
+            cycle_id="test_123",
+            context=context,
+            plan=plan,
+            result=result,
+        )
+        
+        # Verify stored
+        assert "cycles" in module._local_cache
+        assert "test_123" in module._local_cache["cycles"]
+    
+    @pytest.mark.asyncio
+    async def test_store_agent_locally(self):
+        """Test storing agent info in local cache."""
+        from {{cookiecutter.package_name}}.genesis.memory import MemoryModule
+        
+        module = MemoryModule()
+        module._use_local = True
+        
+        await module.store_agent(
+            "test_agent",
+            {"type": "research", "target": "bigquery"},
+        )
+        
+        assert "agents" in module._local_cache
+        assert "test_agent" in module._local_cache["agents"]
+    
+    @pytest.mark.asyncio
+    async def test_get_cycles_empty(self):
+        """Test getting cycles when none exist."""
+        from {{cookiecutter.package_name}}.genesis.memory import MemoryModule
+        
+        module = MemoryModule()
+        module._use_local = True
+        
+        cycles = await module.get_cycles()
+        
+        assert cycles == []
+    
+    @pytest.mark.asyncio
+    async def test_get_metrics(self):
+        """Test getting metrics."""
+        from {{cookiecutter.package_name}}.genesis.memory import MemoryModule
+        
+        module = MemoryModule()
+        module._use_local = True
+        
+        metrics = await module.get_metrics()
+        
+        assert "total_cycles" in metrics
+        assert "success_rate" in metrics
+        assert "avg_cycle_duration_ms" in metrics
+{%- endif %}
diff --git a/{{cookiecutter.project_name}}/tests/genesis/test_genesis_perceive.py b/{{cookiecutter.project_name}}/tests/genesis/test_genesis_perceive.py
new file mode 100644
index 0000000..7bcdfa6
--- /dev/null
+++ b/{{cookiecutter.project_name}}/tests/genesis/test_genesis_perceive.py
@@ -0,0 +1,129 @@
+{%- if cookiecutter.use_google_adk == 'y' and cookiecutter.use_google_cloud == 'y' %}
+"""Tests for GENESIS Perceive module."""
+import pytest
+from unittest.mock import MagicMock, patch, AsyncMock
+
+
+class TestEnvironmentContext:
+    """Tests for EnvironmentContext dataclass."""
+    
+    def test_context_hash_is_consistent(self):
+        """Test that hash is consistent for same data."""
+        from {{cookiecutter.package_name}}.genesis.perceive import EnvironmentContext
+        
+        ctx1 = EnvironmentContext(
+            project_id="test-project",
+            region="us-central1",
+            services=["service1", "service2"],
+        )
+        
+        ctx2 = EnvironmentContext(
+            project_id="test-project",
+            region="us-central1",
+            services=["service1", "service2"],
+        )
+        
+        assert ctx1.hash() == ctx2.hash()
+    
+    def test_context_hash_differs_for_different_data(self):
+        """Test that hash differs for different data."""
+        from {{cookiecutter.package_name}}.genesis.perceive import EnvironmentContext
+        
+        ctx1 = EnvironmentContext(project_id="project1")
+        ctx2 = EnvironmentContext(project_id="project2")
+        
+        assert ctx1.hash() != ctx2.hash()
+    
+    def test_context_to_prompt(self):
+        """Test converting context to prompt."""
+        from {{cookiecutter.package_name}}.genesis.perceive import EnvironmentContext
+        
+        ctx = EnvironmentContext(
+            project_id="test-project",
+            region="us-central1",
+            services=["storage.googleapis.com", "bigquery.googleapis.com"],
+            user_task="Test task",
+        )
+        
+        prompt = ctx.to_prompt()
+        
+        assert "test-project" in prompt
+        assert "us-central1" in prompt
+        assert "Test task" in prompt
+    
+    def test_context_empty_factory(self):
+        """Test empty context creation."""
+        from {{cookiecutter.package_name}}.genesis.perceive import EnvironmentContext
+        
+        ctx = EnvironmentContext.empty()
+        
+        assert ctx.project_id == "unknown"
+        assert len(ctx.changes) > 0
+    
+    def test_context_to_dict(self):
+        """Test context serialization."""
+        from {{cookiecutter.package_name}}.genesis.perceive import EnvironmentContext
+        
+        ctx = EnvironmentContext(
+            project_id="test-project",
+            services=["svc1"],
+        )
+        
+        data = ctx.to_dict()
+        
+        assert data["project_id"] == "test-project"
+        assert "hash" in data
+        assert "timestamp" in data
+
+
+class TestPerceiveModule:
+    """Tests for PerceiveModule."""
+    
+    @pytest.fixture
+    def mock_discovery(self):
+        """Mock GCPDiscovery."""
+        with patch(
+            "{{cookiecutter.package_name}}.genesis.perceive.PerceiveModule.discovery",
+            new_callable=lambda: property(lambda self: self._mock_discovery),
+        ):
+            yield
+    
+    @pytest.mark.asyncio
+    async def test_scan_without_discovery(self):
+        """Test scan works even without GCP discovery."""
+        from {{cookiecutter.package_name}}.genesis.perceive import PerceiveModule
+        
+        # Create module without mocking - will use None discovery
+        module = PerceiveModule()
+        module._discovery = None  # Force no discovery
+        
+        context = await module.scan()
+        
+        assert context is not None
+        assert context.project_id == "unknown"
+    
+    @pytest.mark.asyncio
+    async def test_scan_detects_changes(self):
+        """Test that changes are detected between scans."""
+        from {{cookiecutter.package_name}}.genesis.perceive import PerceiveModule
+        
+        module = PerceiveModule()
+        module._discovery = None
+        
+        # First scan
+        ctx1 = await module.scan()
+        assert any(c.get("type") == "initial_scan" for c in ctx1.changes)
+        
+        # Second scan (would detect changes if any)
+        ctx2 = await module.scan()
+        assert ctx2.changes is not None
+    
+    def test_get_last_context(self):
+        """Test retrieving last scanned context."""
+        from {{cookiecutter.package_name}}.genesis.perceive import PerceiveModule
+        
+        module = PerceiveModule()
+        
+        # Initially None
+        assert module.get_last_context() is None
+{%- endif %}
diff --git a/{{cookiecutter.project_name}}/tests/genesis/test_genesis_think.py b/{{cookiecutter.project_name}}/tests/genesis/test_genesis_think.py
new file mode 100644
index 0000000..de03cc2
--- /dev/null
+++ b/{{cookiecutter.project_name}}/tests/genesis/test_genesis_think.py
@@ -0,0 +1,175 @@
+{%- if cookiecutter.use_google_adk == 'y' and cookiecutter.use_google_cloud == 'y' %}
+"""Tests for GENESIS Think module."""
+import pytest
+from unittest.mock import MagicMock, patch, AsyncMock
+import json
+
+
+class TestAction:
+    """Tests for Action dataclass."""
+    
+    def test_action_to_dict(self):
+        """Test Action serialization."""
+        from {{cookiecutter.package_name}}.genesis.think import Action
+        
+        action = Action(
+            type="generate_agent",
+            target="bigquery",
+            spec={"description": "BQ agent"},
+            priority=5,
+            reasoning="Need BQ support",
+        )
+        
+        data = action.to_dict()
+        
+        assert data["type"] == "generate_agent"
+        assert data["target"] == "bigquery"
+        assert data["priority"] == 5
+    
+    def test_action_from_dict(self):
+        """Test Action deserialization."""
+        from {{cookiecutter.package_name}}.genesis.think import Action
+        
+        data = {
+            "type": "query",
+            "target": "storage",
+            "spec": {"query_type": "list"},
+            "priority": 3,
+            "reasoning": "List buckets",
+        }
+        
+        action = Action.from_dict(data)
+        
+        assert action.type == "query"
+        assert action.target == "storage"
+        assert action.priority == 3
+
+
+class TestActionPlan:
+    """Tests for ActionPlan dataclass."""
+    
+    def test_action_plan_from_json(self):
+        """Test ActionPlan creation from JSON."""
+        from {{cookiecutter.package_name}}.genesis.think import ActionPlan
+        
+        data = {
+            "reasoning": "Test reasoning",
+            "confidence": 0.8,
+            "actions": [
+                {"type": "query", "target": "test", "priority": 1},
+                {"type": "deploy", "target": "app", "priority": 5},
+            ],
+        }
+        
+        plan = ActionPlan.from_json(data)
+        
+        assert plan.reasoning == "Test reasoning"
+        assert plan.confidence == 0.8
+        assert len(plan.actions) == 2
+    
+    def test_action_plan_get_actions_by_priority(self):
+        """Test actions are sorted by priority."""
+        from {{cookiecutter.package_name}}.genesis.think import ActionPlan, Action
+        
+        plan = ActionPlan(
+            reasoning="Test",
+            actions=[
+                Action(type="a", target="1", priority=1),
+                Action(type="b", target="2", priority=10),
+                Action(type="c", target="3", priority=5),
+            ],
+        )
+        
+        sorted_actions = plan.get_actions_by_priority()
+        
+        assert sorted_actions[0].priority == 10
+        assert sorted_actions[1].priority == 5
+        assert sorted_actions[2].priority == 1
+    
+    def test_action_plan_empty(self):
+        """Test empty plan creation."""
+        from {{cookiecutter.package_name}}.genesis.think import ActionPlan
+        
+        plan = ActionPlan.empty()
+        
+        assert len(plan.actions) == 0
+        assert plan.confidence == 0.0
+
+
+class TestThinkModule:
+    """Tests for ThinkModule."""
+    
+    def test_extract_json_from_raw(self):
+        """Test JSON extraction from raw response."""
+        from {{cookiecutter.package_name}}.genesis.think import ThinkModule
+        
+        module = ThinkModule()
+        
+        raw_json = '{"reasoning": "test", "actions": []}'
+        result = module._extract_json(raw_json)
+        
+        assert result is not None
+        data = json.loads(result)
+        assert data["reasoning"] == "test"
+    
+    def test_extract_json_from_markdown(self):
+        """Test JSON extraction from markdown code block."""
+        from {{cookiecutter.package_name}}.genesis.think import ThinkModule
+        
+        module = ThinkModule()
+        
+        markdown_json = '''Here is the plan:
+```json
+{"reasoning": "test", "actions": []}
+```
+'''
+        result = module._extract_json(markdown_json)
+        
+        assert result is not None
+        data = json.loads(result)
+        assert data["reasoning"] == "test"
+    
+    def test_clean_code_removes_markdown(self):
+        """Test code cleaning removes markdown."""
+        from {{cookiecutter.package_name}}.genesis.think import ThinkModule
+        
+        module = ThinkModule()
+        
+        code_with_markdown = '''```python
+def hello():
+    print("Hello")
+```'''
+        
+        clean = module._clean_code(code_with_markdown)
+        
+        assert "```" not in clean
+        assert "def hello():" in clean
+    
+    def test_validate_syntax_accepts_valid_code(self):
+        """Test syntax validation accepts valid Python."""
+        from {{cookiecutter.package_name}}.genesis.think import ThinkModule
+        
+        module = ThinkModule()
+        
+        valid_code = '''
+def hello(name: str) -> str:
+    """Say hello."""
+    return f"Hello, {name}"
+'''
+        # Should not raise
+        module._validate_syntax(valid_code)
+    
+    def test_validate_syntax_rejects_invalid_code(self):
+        """Test syntax validation rejects invalid Python."""
+        from {{cookiecutter.package_name}}.genesis.think import ThinkModule
+        
+        module = ThinkModule()
+        
+        invalid_code = '''
+def broken(
+    print("missing closing paren"
+'''
+        
+        with pytest.raises(SyntaxError):
+            module._validate_syntax(invalid_code)
+{%- endif %}
